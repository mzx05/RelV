# -*- coding: utf-8 -*-
"""RelV(V3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZPEq4A8Sfn4J97YHw_x0f2f6q5pnrOOs
"""

# Install necessary libraries with CUDA support
!pip install -q torch

import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():   print(f"GPU: {torch.cuda.get_device_name(0)}")

# Check CUDA version first
!nvcc --version

# Install llama-cpp-python with CUDA 12.x support
!pip install --no-cache-dir llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123

# Install LlamaIndex
!pip install llama-index

from llama_cpp import Llama
import os
# Define model path
model_path = "/content/mistral.gguf"

# Download Mistral model if not already present
if not os.path.exists(model_path):
    !wget "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" -O {model_path}
print(f"Model downloaded to {model_path}")

"""# Define model path
model_path = "/content/tinyllama.gguf"

# Download Mistral model if not already present
if not os.path.exists(model_path):
      !wget "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf" -O {model_path}
print(f"Model downloaded to {model_path}")"""

# Verify model file exists
if os.path.exists(model_path):   print(f"Model file exists. Size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
else:   print("Model file not found!")

# Load the model with GPU acceleration
try:
  llm = Llama(model_path=model_path,     n_gpu_layers=1,  n_ctx=2048, verbose=True)
  print("Model loaded successfully!")

except Exception as e:   print(f"Error loading model: {e}")

import numpy as np
import torch
from typing import Dict, List, Tuple, Optional, Union
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from dataclasses import dataclass
import uuid
import time
import re

@dataclass
class Node:
    """Represents a node in the RelV database."""
    id: str
    content: str
    embedding: np.ndarray
    metadata: Dict = None
    age: int = 0
    creation_time: float = None

    def __init__(self, content: str, embedding: np.ndarray, node_id: str = None, metadata: Dict = None, age: int = 0):
        self.id = node_id if node_id else str(uuid.uuid4())
        self.content = content
        self.embedding = embedding
        self.metadata = metadata if metadata else {}
        self.age = age
        self.creation_time = time.time()

class RelV:
    """
    Dynamic Relational Vector Database with Adjacency Matrix Representation
    Optimized for SLMs with small context windows and external memory for LLMs
    """

    def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.3, chunk_size: int = 512, overlap: int = 50):
        """
        Initialize a new RelV database

        Args:
            embedding_dim: Dimension of the vector embeddings
            similarity_threshold: Threshold for considering nodes as related
            chunk_size: Size of text chunks for processing
            overlap: Overlap between chunks
        """
        self.nodes: Dict[str, Node] = {}
        self.adjacency_matrix = np.zeros((0, 0), dtype=float)
        self.embedding_dim = embedding_dim
        self.similarity_threshold = similarity_threshold
        self.graph = nx.Graph()
        self.chunk_size = chunk_size
        self.overlap = overlap

    def add_node(self, content: str, embedding: np.ndarray, metadata: Dict = None) -> str:
        """
        Add a new node to the database

        Args:
            content: Text content of the node
            embedding: Vector embedding of the content
            metadata: Additional metadata for the node

        Returns:
            ID of the newly created node
        """
        # Create a new node
        node = Node(content=content, embedding=embedding, metadata=metadata)
        node_id = node.id

        # Add node to the collection
        self.nodes[node_id] = node

        # Update adjacency matrix
        if len(self.nodes) == 1:
            # First node, initialize adjacency matrix
            self.adjacency_matrix = np.zeros((1, 1), dtype=float)
        else:
            # Calculate similarities with existing nodes
            old_size = len(self.nodes) - 1
            new_matrix = np.zeros((old_size + 1, old_size + 1), dtype=float)
            new_matrix[:old_size, :old_size] = self.adjacency_matrix

            # Calculate similarities between the new node and all existing nodes
            for i, existing_id in enumerate(list(self.nodes.keys())[:-1]):
                existing_node = self.nodes[existing_id]
                sim = self._calculate_similarity(node.embedding, existing_node.embedding)
                new_matrix[i, old_size] = sim
                new_matrix[old_size, i] = sim

                # Update graph if nodes are connected
                if sim >= self.similarity_threshold:
                    self.graph.add_edge(node_id, existing_id, weight=sim)

            self.adjacency_matrix = new_matrix

        # Add node to graph
        self.graph.add_node(node_id)

        return node_id

    def _calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Calculate cosine similarity between two embeddings"""
        e1 = embedding1.reshape(1, -1)
        e2 = embedding2.reshape(1, -1)
        return cosine_similarity(e1, e2)[0][0]

    def get_related_nodes(self, node_id: str, k: int = 5, min_similarity: float = None) -> List[Tuple[str, float]]:
        """
        Find the k most related nodes to the given node

        Args:
            node_id: ID of the node to find relations for
            k: Number of related nodes to return
            min_similarity: Minimum similarity threshold (defaults to class threshold if None)

        Returns:
            List of tuples containing (node_id, similarity_score) sorted by similarity
        """
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")

        threshold = min_similarity if min_similarity is not None else self.similarity_threshold
        node_index = list(self.nodes.keys()).index(node_id)

        # Get similarities from adjacency matrix
        similarities = [(i, self.adjacency_matrix[node_index, i])
                       for i in range(len(self.nodes)) if i != node_index]

        # Filter by threshold and sort
        filtered_similarities = [(list(self.nodes.keys())[i], sim)
                                for i, sim in similarities if sim >= threshold]
        sorted_similarities = sorted(filtered_similarities, key=lambda x: x[1], reverse=True)

        return sorted_similarities[:k]

    def query_by_embedding(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[str, float]]:
        """
        Find the k most similar nodes to the given query embedding

        Args:
            query_embedding: Query vector
            k: Number of results to return

        Returns:
            List of tuples containing (node_id, similarity_score) sorted by similarity
        """
        similarities = []

        for node_id, node in self.nodes.items():
            sim = self._calculate_similarity(query_embedding, node.embedding)
            similarities.append((node_id, sim))

        # Sort by similarity (descending)
        sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)

        return sorted_similarities[:k]

    def get_node_content(self, node_id: str) -> str:
        """Get the content of a node by its ID"""
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")
        return self.nodes[node_id].content

    def get_node(self, node_id: str) -> Node:
        """Get a node by its ID"""
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")
        return self.nodes[node_id]

    def get_path_between_nodes(self, start_id: str, end_id: str) -> List[str]:
        """Find the shortest path between two nodes"""
        if start_id not in self.nodes or end_id not in self.nodes:
            raise ValueError("One or both node IDs not found")

        try:
            path = nx.shortest_path(self.graph, start_id, end_id)
            return path
        except nx.NetworkXNoPath:
            return []

    def remove_node(self, node_id: str) -> bool:
        """Remove a node from the database"""
        if node_id not in self.nodes:
            return False

        # Get index of node to remove
        node_index = list(self.nodes.keys()).index(node_id)

        # Remove node from collection
        del self.nodes[node_id]

        # Update adjacency matrix
        indices = list(range(len(self.adjacency_matrix)))
        indices.remove(node_index)
        self.adjacency_matrix = self.adjacency_matrix[np.ix_(indices, indices)]

        # Remove from graph
        self.graph.remove_node(node_id)

        return True

    def get_community(self, node_id: str, depth: int = 2) -> List[str]:
        """Get all nodes connected to the given node up to a certain depth"""
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")

        # Use BFS to find nodes up to given depth
        visited = {node_id}
        queue = [(node_id, 0)]  # (node_id, depth)
        community = []

        while queue:
            current_id, current_depth = queue.pop(0)
            if current_depth > 0:  # Don't include the source node
                community.append(current_id)

            if current_depth < depth:
                for neighbor in self.graph.neighbors(current_id):
                    if neighbor not in visited:
                        visited.add(neighbor)
                        queue.append((neighbor, current_depth + 1))

        return community

    def get_stats(self) -> Dict:
        """Get statistics about the database"""
        return {
            "node_count": len(self.nodes),
            "edge_count": self.graph.number_of_edges(),
            "density": nx.density(self.graph),
            "connected_components": nx.number_connected_components(self.graph),
            "avg_clustering": nx.average_clustering(self.graph)
        }

    def update_node_ages(self, max_age: int = 10, delete_old_nodes: bool = False) -> List[str]:
        """
        Update age of all nodes and optionally remove nodes that exceed max_age

        Args:
            max_age: Maximum age for a node before considering deletion
            delete_old_nodes: Whether to delete nodes that exceed max_age

        Returns:
            List of removed node IDs (if any)
        """
        removed_nodes = []
        nodes_to_remove = []

        # Update age for all nodes
        for node_id, node in self.nodes.items():
            # Increment age
            node.age += 1

            # Check if node should be deleted
            if delete_old_nodes and node.age > max_age:
                # Only delete conversation nodes, not document nodes
                is_conversation = node.metadata and node.metadata.get("type") == "conversation"
                if is_conversation:
                    nodes_to_remove.append(node_id)

        # Remove old nodes
        for node_id in nodes_to_remove:
            self.remove_node(node_id)
            removed_nodes.append(node_id)

        return removed_nodes

    def reset_node_age(self, node_id: str):
        """
        Reset the age of a node to 0, typically called when a node is accessed

        Args:
            node_id: ID of the node to reset
        """
        if node_id in self.nodes:
            self.nodes[node_id].age = 0

    def reset_related_node_ages(self, node_id: str, k: int = 5):
        """
        Reset ages for a node and its k most related nodes

        Args:
            node_id: ID of the primary node
            k: Number of related nodes to reset
        """
        # Reset the primary node
        self.reset_node_age(node_id)

        # Get related nodes and reset their ages too
        related_nodes = self.get_related_nodes(node_id, k=k)
        for related_id, _ in related_nodes:
            self.reset_node_age(related_id)

    def _chunk_text(self, text: str) -> List[str]:
        """
        Split text into chunks with overlap in an efficient way
        Optimized for large documents
        """
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []

        # Precomputing break points for efficiency with large documents
        # Find all sentence-ending characters
        break_points = []
        for match in re.finditer(r'[.!?\n]', text):
            break_points.append(match.start() + 1)  # Add 1 to include the punctuation

        # If no break points found, fall back to character-based chunking
        if not break_points:
            return [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size - self.overlap)]

        # Process chunks
        start = 0
        while start < len(text):
            # Find end of chunk
            end = min(start + self.chunk_size, len(text))

            # If not at end of text, find a good breaking point
            if end < len(text):
                # Look for the largest break point that's within our chunk size limit
                suitable_break_points = [bp for bp in break_points if bp > start and bp <= end]

                if suitable_break_points:
                    # Find break points in the last 20% of the chunk for a clean break
                    last_20_percent = end - int(0.2 * self.chunk_size)
                    ideal_break_points = [bp for bp in suitable_break_points if bp >= last_20_percent]

                    if ideal_break_points:
                        end = ideal_break_points[-1]  # Take the last one in the range
                    else:
                        end = suitable_break_points[-1]  # Take the last available break point

            # Add chunk
            chunks.append(text[start:end])

            # Calculate next start position with overlap
            start = max(0, end - self.overlap)

        return chunks
    def chunk_text_efficient(self, text: str) -> List[str]:
        """
        Split text into chunks with overlap using a more efficient algorithm
        This method reduces computational overhead by using fixed indices and minimizing list operations

        Args:
            text: The text to chunk

        Returns:
            List of text chunks
        """
        # Quick return for small texts
        if len(text) <= self.chunk_size:
            return [text]

        # Initialize variables
        text_length = len(text)
        chunks = []

        # Pre-calculate sentence break indices to avoid expensive regex operations in loops
        # Use a more efficient regex pattern without backtracking
        break_indices = [match.start() + 1 for match in re.finditer(r'[.!?\n]', text)]

        # Handle case with no sentence breaks
        if not break_indices:
            # Use an efficient range-based chunking with fixed indices
            chunks = []
            for i in range(0, text_length, self.chunk_size - self.overlap):
                end_idx = min(i + self.chunk_size, text_length)
                chunks.append(text[i:end_idx])
            return chunks

        # Create chunk boundaries in advance (start and end positions for each chunk)
        # This avoids recalculating positions in the loop
        chunk_boundaries = []
        position = 0

        while position < text_length:
            # Calculate the nominal end position
            end_position = min(position + self.chunk_size, text_length)

            # If we're not at the end, find a good break point
            if end_position < text_length:
                # Define the region where we prefer to break (last 20% of chunk)
                break_region_start = end_position - int(0.2 * self.chunk_size)

                # Find a suitable break point using binary search (much faster than list comprehension)
                suitable_break = end_position  # Default to the calculated end

                # Use binary search to find the rightmost break point before end_position
                left, right = 0, len(break_indices) - 1
                while left <= right:
                    mid = (left + right) // 2
                    if break_indices[mid] <= end_position:
                        left = mid + 1
                    else:
                        right = mid - 1

                # If we found a break point before end_position
                if right >= 0:
                    # Now find the rightmost break point in our preferred region
                    ideal_break_index = right
                    while ideal_break_index >= 0 and break_indices[ideal_break_index] >= break_region_start:
                        ideal_break_index -= 1

                    # If we found a break in the preferred region, use it
                    # Otherwise use the last break point before end_position
                    if ideal_break_index < right and ideal_break_index >= 0 and break_indices[ideal_break_index] >= break_region_start:
                        suitable_break = break_indices[ideal_break_index]
                    else:
                        suitable_break = break_indices[right]

            # Store the boundary for this chunk
            chunk_boundaries.append((position, suitable_break))

            # Move to next position with overlap
            position = suitable_break - self.overlap
            if position < 0:
                position = 0

            # Avoid infinite loops if we couldn't make progress
            if position >= text_length or (len(chunk_boundaries) > 1 and
                                          chunk_boundaries[-1] == chunk_boundaries[-2]):
                break

        # Create chunks from pre-calculated boundaries
        chunks = [text[start:end] for start, end in chunk_boundaries]

        return chunks

import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer
import difflib

class RelVContextManager:
    """
    Manages context for LLMs using the RelV database for dynamic memory
    """

    def __init__(self,
                 relv_db: 'RelV',
                 llm: Any,
                 embedding_model: Optional[SentenceTransformer] = None,
                 max_context_size: int = 2048):
        """
        Initialize the RelV Context Manager

        Args:
            relv_db: RelV database instance
            llm: Language model instance (e.g., Llama model)
            embedding_model: Model to create embeddings (if None, must provide embeddings)
            max_context_size: Maximum context size for the LLM
        """
        self.relv_db = relv_db
        self.llm = llm
        self.embedding_model = embedding_model
        self.max_context_size = max_context_size
        self.query_history = []
    def add_conversation_to_memory(self, query: str, response: str) -> str:
              """
              Add a conversation exchange to the RelV memory

              Args:
                  query: User query
                  response: LLM response

              Returns:
                  Node ID of the stored conversation
              """
              conversation = f"User: {query}\nModel Response: {response}"

              if self.embedding_model:
                  embedding = self.embedding_model.encode(conversation)
              else:
                  raise ValueError("Embedding model not provided. Cannot embed conversation.")

              # Add to database with specific conversation metadata
              node_id = self.relv_db.add_node(
                  content=conversation,
                  embedding=embedding,
                  metadata={
                      "type": "conversation",
                      "timestamp": time.time(),
                      "query": query,
                      "is_triage_eligible": True  # Mark as eligible for age-based triage/removal
                  }
              )

              # Automatically run maintenance after adding conversations
              # to ensure the database doesn't grow too large
              if len(self.query_history) % 10 == 0:  # Every 10 queries
                  print("Running scheduled database maintenance...")
                  self.maintain_database()

              return node_id
    def add_document(self, document: str, metadata: Dict = None, batch_size: int = 8, use_efficient_chunking: bool = True) -> List[str]:
        """
        Add a document to the RelV database by chunking and embedding
        Optimized for large documents using batch processing

        Args:
            document: Text document to add
            metadata: Additional metadata for the document
            batch_size: Number of chunks to process in each batch for efficiency
            use_efficient_chunking: Whether to use the more efficient chunking algorithm

        Returns:
            List of node IDs created
        """
        if not document:
            return []

        # Start time for performance tracking
        start_time = time.time()

        # Chunk the document using the method from RelV class
        if use_efficient_chunking:
            chunks = self.relv_db.chunk_text_efficient(document)
        else:
            chunks = self.relv_db._chunk_text(document)

        total_chunks = len(chunks)
        print(f"Document chunked into {total_chunks} parts")

        node_ids = []

        # Process in batches to avoid memory issues with large documents
        for batch_idx in range(0, total_chunks, batch_size):
            batch_end = min(batch_idx + batch_size, total_chunks)
            current_batch = chunks[batch_idx:batch_end]

            # Create batch embeddings in one call for efficiency
            if self.embedding_model:
                try:
                    # Use model's batch processing capabilities
                    batch_embeddings = self.embedding_model.encode(current_batch)
                except Exception as e:
                    print(f"Batch embedding failed: {e}, falling back to individual processing")
                    # Fallback to individual processing if batch fails
                    batch_embeddings = []
                    for chunk in current_batch:
                        batch_embeddings.append(self.embedding_model.encode(chunk))
            else:
                raise ValueError("Embedding model not provided. Cannot embed text.")

            # Add each chunk to the database
            for i, (chunk, embedding) in enumerate(zip(current_batch, batch_embeddings)):
                global_idx = batch_idx + i

                # Create chunk-specific metadata
                chunk_metadata = {
                    "chunk_index": global_idx,
                    "total_chunks": total_chunks,
                    "source": "document"
                }

                # Merge with provided metadata
                if metadata:
                    chunk_metadata.update(metadata)

                # Add to database
                node_id = self.relv_db.add_node(
                    content=chunk,
                    embedding=embedding,
                    metadata=chunk_metadata
                )

                node_ids.append(node_id)

            # Progress report
            if total_chunks > batch_size * 2:  # Only show for larger documents
                print(f"Processed {batch_end}/{total_chunks} chunks ({(batch_end/total_chunks)*100:.1f}%)")

        # Performance tracking
        elapsed_time = time.time() - start_time
        print(f"Document processing completed in {elapsed_time:.2f} seconds")
        print(f"Average time per chunk: {elapsed_time/total_chunks:.4f} seconds")

        return node_ids

    def query_optimized(self, query: str, k: int = 3, cache_results: bool = True,
                use_batched_context: bool = True, temperature: float = 0.1) -> str:
        """
        Process a query using the RelV database and LLM with optimizations
        to avoid slow llama_cpp.llama_decode bottlenecks

        Args:
            query: User query
            k: Number of related nodes to include in context
            cache_results: Whether to cache results to avoid redundant LLM calls
            use_batched_context: Whether to batch context processing for better performance
            temperature: Temperature for LLM sampling

        Returns:
            Response from the LLM
        """
        # Check query cache first (if enabled)
        if cache_results and hasattr(self, 'query_cache'):
            # Simple cache lookup
            for cached_query, cached_response in self.query_cache.items():
                # Use string similarity to find similar cached queries
                if len(query) > 0 and len(cached_query) > 0:
                    similarity = difflib.SequenceMatcher(None, query.lower(), cached_query.lower()).ratio()
                    if similarity > 0.9:  # 90% similarity threshold
                        print(f"Using cached response (similarity: {similarity:.2f})")
                        return cached_response

        # Initialize cache if it doesn't exist
        if cache_results and not hasattr(self, 'query_cache'):
            self.query_cache = {}

        # Store query in history
        self.query_history.append(query)
        start_time = time.time()

        # Get embedding for the query
        if self.embedding_model:
            query_embedding = self.embedding_model.encode(query)
        else:
            raise ValueError("Embedding model not provided. Cannot embed query.")

        # Find relevant nodes
        relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=k)

        # Early exit if no relevant nodes
        if not relevant_nodes:
            response = self.llm(query, max_tokens=512, temperature=temperature)
            result = response["choices"][0]["text"]

            # Store in cache
            if cache_results:
                self.query_cache[query] = result

            return result

        # Reset ages for nodes being used
        for node_id, _ in relevant_nodes:
            self.relv_db.reset_node_age(node_id)

        # Build context more efficiently
        if use_batched_context:
            # Get all node contents at once
            node_contents = [(node_id, self.relv_db.get_node_content(node_id), similarity)
                            for node_id, similarity in relevant_nodes]

            # Calculate total character count
            total_chars = sum(len(content) for _, content, _ in node_contents)

            # Estimate tokens (roughly 4 chars per token)
            estimated_tokens = total_chars // 4

            # If would exceed context window, reduce content
            if estimated_tokens > (self.max_context_size - 512):  # Leave room for response
                # Sort by similarity (most relevant first)
                node_contents.sort(key=lambda x: x[2], reverse=True)

                # Keep adding until we reach the limit
                selected_contents = []
                current_chars = 0

                for node_id, content, similarity in node_contents:
                    content_chars = len(content)
                    if current_chars + content_chars < (self.max_context_size - 512) * 4:
                        selected_contents.append((node_id, content, similarity))
                        current_chars += content_chars
                    else:
                        # If content is too large, truncate it
                        space_left = (self.max_context_size - 512) * 4 - current_chars
                        if space_left > 100:  # Only include if enough space for meaningful content
                            truncated = content[:space_left]
                            selected_contents.append((node_id, truncated, similarity))
                        break

                node_contents = selected_contents

            # Build context string in one operation
            context_parts = [f"[Relevant Information (Similarity: {similarity:.2f})]\n{content}\n"
                           for _, content, similarity in node_contents]

            context_str = "\n".join(context_parts)
        else:
            # Original approach
            context = []
            total_length = 0

            for node_id, similarity in relevant_nodes:
                node_content = self.relv_db.get_node_content(node_id)

                # Check if adding this content would exceed max context size
                if total_length + len(node_content) + len(query) + 100 > self.max_context_size:
                    break

                context.append(f"[Relevant Information (Similarity: {similarity:.2f})]\n{node_content}\n")
                total_length += len(node_content) + 100  # Add some buffer for formatting

            context_str = "\n".join(context)

        # Create prompt with context
        prompt = f"""The following information is relevant to the query:

{context_str}

Based on the above information, please answer the following query:
{query}
"""

        # Measure context preparation time
        context_prep_time = time.time() - start_time
        llm_start_time = time.time()

        # Query the LLM with optimized parameters
        try:
            # Use a timeout to prevent hanging
            response = self.llm(
                prompt,
                max_tokens=512,
                temperature=temperature,
                stop=["<|im_end|>"],  # Add stop tokens to prevent overgeneration
                top_p=0.9,  # Add top_p to limit token selection
                repeat_penalty=1.1  # Slight penalty for repetition
            )

            result = response["choices"][0]["text"]

            # Store in cache
            if cache_results:
                self.query_cache[query] = result

            # Log performance metrics
            llm_time = time.time() - llm_start_time
            total_time = time.time() - start_time
            print(f"Query processing: {context_prep_time:.2f}s context prep, {llm_time:.2f}s LLM, {total_time:.2f}s total")

            return result

        except Exception as e:
            print(f"Error during LLM query: {e}")
            # Fallback approach: simpler prompt
            simplified_prompt = f"Query: {query}\nPlease provide a concise answer."
            response = self.llm(simplified_prompt, max_tokens=256, temperature=temperature)
            return response["choices"][0]["text"]
    def query(self, query: str, k: int = 3) -> str:
        """
        Process a query using the RelV database and LLM
        Note: Consider using query_optimized for better performance

        Args:
            query: User query
            k: Number of related nodes to include in context

        Returns:
            Response from the LLM
        """
        # For backward compatibility, use the optimized version by default
        import difflib  # Required for the optimized version
        return self.query_optimized(query, k=k)


    def get_dynamic_context(self, query: str, history_weight: float = 0.3, k_recent: int = 2, k_relevant: int = 3) -> List[str]:
        """
        Build a dynamic context using both relevance and recency

        Args:
            query: Current query
            history_weight: Weight given to conversation history vs relevance
            k_recent: Number of recent conversations to include
            k_relevant: Number of relevant nodes to include

        Returns:
            List of node contents to use as context
        """
        # Get embedding for the query
        if self.embedding_model:
            query_embedding = self.embedding_model.encode(query)
        else:
            raise ValueError("Embedding model not provided. Cannot embed query.")

        # Get relevant nodes
        relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=k_relevant)

        # Find recent conversation nodes
        conversation_nodes = {node_id: node for node_id, node in self.relv_db.nodes.items()
                             if node.metadata and node.metadata.get("type") == "conversation"}

        # Sort by recency (assuming node IDs are chronological)
        recent_nodes = list(conversation_nodes.items())[-k_recent:]

        # Combine information sources
        context_nodes = []

        # Add recent conversation nodes
        for node_id, node in recent_nodes:
            context_nodes.append((node_id, history_weight))

        # Add relevant nodes
        for node_id, similarity in relevant_nodes:
            # Skip if already included
            if node_id in [n[0] for n in context_nodes]:
                continue
            context_nodes.append((node_id, (1 - history_weight) * similarity))

        # Sort by combined weight
        context_nodes.sort(key=lambda x: x[1], reverse=True)

        # Get node contents
        context_contents = []
        total_length = 0

        for node_id, _ in context_nodes:
            # Reset age for nodes being used
            self.relv_db.reset_node_age(node_id)

            content = self.relv_db.get_node_content(node_id)

            # Check if adding this would exceed max context size
            if total_length + len(content) + len(query) + 100 > self.max_context_size:
                break

            context_contents.append(content)
            total_length += len(content) + 20  # Add some buffer

        return context_contents

    def maintain_database(self, max_age: int = 30, auto_cleanup: bool = True) -> Dict:
        """
        Perform maintenance on the database - update ages and clean up old nodes

        Args:
            max_age: Maximum age before considering a node for removal
            auto_cleanup: Whether to automatically remove old nodes

        Returns:
            Dictionary with maintenance statistics
        """
        # Increment ages for all nodes
        start_time = time.time()
        removed_nodes = self.relv_db.update_node_ages(max_age=max_age, delete_old_nodes=auto_cleanup)

        stats = {
            "total_nodes": len(self.relv_db.nodes),
            "nodes_removed": len(removed_nodes),
            "time_taken": time.time() - start_time
        }

        # Get age distribution
        age_distribution = {}
        for node in self.relv_db.nodes.values():
            age = node.age
            if age not in age_distribution:
                age_distribution[age] = 0
            age_distribution[age] += 1

        stats["age_distribution"] = age_distribution

        print(f"Database maintenance completed. {len(removed_nodes)} nodes removed.")
        print(f"Current node count: {stats['total_nodes']}")

        return stats

        def batch_query(self, queries: List[str], k: int = 3, cache_results: bool = True) -> List[str]:

          if not queries:
              return []

          # Apply inference optimizations before batch processing
          self.optimize_inference_settings()

          # Get all query embeddings at once
          if self.embedding_model:
              try:
                  # Use batch embedding
                  query_embeddings = self.embedding_model.encode(queries)
              except:
                  # Fallback to individual embedding
                  query_embeddings = [self.embedding_model.encode(q) for q in queries]
          else:
              raise ValueError("Embedding model not provided. Cannot embed queries.")

          # Find all relevant nodes for all queries at once
          all_relevant_nodes = []
          for query_embedding in query_embeddings:
              relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=k*2)  # Get more nodes initially
              all_relevant_nodes.append(relevant_nodes)

          # Reset ages for all used nodes
          used_node_ids = set()
          for relevant_nodes in all_relevant_nodes:
              for node_id, _ in relevant_nodes:
                  used_node_ids.add(node_id)

          for node_id in used_node_ids:
              self.relv_db.reset_node_age(node_id)

          # Process each query with context
          responses = []

          for i, query in enumerate(queries):
              # Check for cached response first
              if cache_results and hasattr(self, 'query_cache') and query in self.query_cache:
                  responses.append(self.query_cache[query])
                  continue

              # Use optimized query
              response = self.query_optimized(
                  query,
                  k=k,
                  cache_results=cache_results,
                  use_batched_context=True,
                  temperature=0.1
              )

              responses.append(response)

          return responses

    def benchmark_query_performance(self, test_query: str, runs: int = 3) -> Dict:
        """
        Benchmark the performance difference between original and optimized query methods

        Args:
            test_query: Query to use for benchmarking
            runs: Number of benchmark runs

        Returns:
            Dictionary with benchmark results
        """
        import difflib  # Required for the optimized version

        results = {
            "original": {"times": []},
            "optimized": {"times": []},
            "speedup": 0
        }

        print(f"Benchmarking query performance with {runs} runs...")

        # Define the original query method for comparison
        def original_query(query, k=3):
            # Store query in history
            self.query_history.append(query)

            # Get embedding for the query
            query_embedding = self.embedding_model.encode(query)

            # Find relevant nodes
            relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=k)

            # Build context from relevant nodes
            context = []
            total_length = 0

            for node_id, similarity in relevant_nodes:
                node_content = self.relv_db.get_node_content(node_id)
                if total_length + len(node_content) + len(query) + 100 > self.max_context_size:
                    break
                context.append(f"[Relevant Information (Similarity: {similarity:.2f})]\n{node_content}\n")
                total_length += len(node_content) + 100

            # Create prompt with context
            prompt = f"The following information is relevant to the query:\n\n{' '.join(context)}\n\nBased on the above information, please answer the following query:\n{query}"

            # Query the LLM
            response = self.llm(prompt, max_tokens=512, temperature=0.1)
            return response["choices"][0]["text"]

        # Run original method
        print("Testing original query method...")
        for i in range(runs):
            start_time = time.time()
            _ = original_query(test_query)
            elapsed = time.time() - start_time
            results["original"]["times"].append(elapsed)
            print(f"  Run {i+1}/{runs}: {elapsed:.4f}s")

        # Run optimized method
        print("Testing optimized query method...")
        for i in range(runs):
            start_time = time.time()
            _ = self.query_optimized(test_query, cache_results=False)  # Disable caching for benchmark
            elapsed = time.time() - start_time
            results["optimized"]["times"].append(elapsed)
            print(f"  Run {i+1}/{runs}: {elapsed:.4f}s")

        # Calculate averages
        results["original"]["avg_time"] = sum(results["original"]["times"]) / runs
        results["optimized"]["avg_time"] = sum(results["optimized"]["times"]) / runs

        # Calculate speedup
        speedup = results["original"]["avg_time"] / results["optimized"]["avg_time"]
        results["speedup"] = speedup

        print("\nBenchmark Results:")
        print(f"Original method average time: {results['original']['avg_time']:.4f}s")
        print(f"Optimized method average time: {results['optimized']['avg_time']:.4f}s")
        print(f"Speedup factor: {speedup:.2f}x")

        return results

# Install additional required packages
!pip install -q sentence-transformers networkx scikit-learn

# Import necessary libraries
import torch
import numpy as np
from llama_cpp import Llama
import os
from sentence_transformers import SentenceTransformer

# Load the embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
print("Embedding model loaded successfully!")

# Initialize RelV database
relv_db = RelV(embedding_dim=384, chunk_size=512, overlap=50)  # MiniLM has 384 dimensions

# Initialize context manager
context_manager = RelVContextManager(
    relv_db=relv_db,
    llm=llm,
    embedding_model=embedding_model,
    max_context_size=2048
)

# Example: Add some documents to the database
documents = [
    """Retrieval-Augmented Generation (RAG) is a technique that enhances large language models by combining them with retrieval systems.
    These systems fetch relevant information from external knowledge sources to supplement the model's internal knowledge,
    providing more accurate and up-to-date responses.""",

    """The adjacency matrix is a square matrix used to represent a finite graph. The elements of the matrix indicate
    whether pairs of vertices are adjacent or not in the graph. For a simple graph with no self-loops,
    the adjacency matrix must have 0s on the diagonal.""",

    """Vector databases store data as high-dimensional vectors and support similarity search operations.
    They are optimized for machine learning applications, particularly those involving embeddings.
    Popular vector databases include Pinecone, Milvus, and FAISS.""",

    """Small Language Models (SLMs) are compact neural networks designed for natural language processing tasks.
    Despite their smaller size compared to large language models (LLMs), they can be effective for specific applications,
    especially when computational resources are limited.""",

    """THIS QUITCLAIM DBED by and between WILMER EARL HEDRICK, unmarried, party of the first part, and PERRY FRANCIS KLUMB and MARY JANE KLUMB, husband and wife,
parties of the second part, whose address is 3608 Willow way, Louisville, Kentucky, 40299.
WITNESSETHs That for the consideration of $1.00 and the settlement of Civil Action No. 85 CI-10019, Jefferson
Circuit Court, Fourth Division, the receipt of which is hereby acknowledged, the party of the first part does hereby
yemise, release and quitclaim unto the parties of the second part, for their joint lives with remainder to the survivor
of them, all of his interest in and to the following de-scribed property situated in Jeffergon County, Kentucky, and
more particularly described as follows, to-wit:BEGINNING at a set pin in the Northwesterly corner of Lot 4, set pin being common to the Southwester-
ly corner of Lot 5 of WILLOW TERRACE """

]

# Add documents to RelV
print("Adding documents to RelV database...")
node_ids = []
for i, doc in enumerate(documents):
    print(f"Adding document {i+1}/{len(documents)}...")
    doc_nodes = context_manager.add_document(doc, metadata={"source": f"document_{i}"})
    node_ids.extend(doc_nodes)

print(f"Added {len(node_ids)} nodes to the database")

# Display database statistics
print("\nRelV Database Statistics:")
stats = relv_db.get_stats()
for key, value in stats.items():
    print(f"{key}: {value}")

# Example query
query = "What is the relationship between RAG and vector databases?"
print(f"\nProcessing query: {query}")

response = context_manager.query(query, k=2)
print("\nResponse:")
print(response)

# Add conversation to memory
node_id = context_manager.add_conversation_to_memory(query, response)
print(f"Conversation added to memory with node ID: {node_id}")

# Try another query that benefits from previous context
follow_up_query = "How can small language models benefit from this approach?"
print(f"\nProcessing follow-up query: {follow_up_query}")

# Get dynamic context
context = context_manager.get_dynamic_context(follow_up_query)
print(f"Dynamic context contains {len(context)} nodes")

# Process the follow-up query
follow_up_response = context_manager.query(follow_up_query, k=4)
print("\nFollow-up Response:")
print(follow_up_response)

# Display related nodes for a specific node
print("\nFinding related nodes...")
if node_ids:
    related = relv_db.get_related_nodes(node_ids[0], k=2)
    print(f"Nodes related to {node_ids[0]}:")
    for rel_id, sim in related:
        print(f"Node: {rel_id}, Similarity: {sim:.4f}")
        print(f"Content: {relv_db.get_node_content(rel_id)[:600]}...")

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
from sklearn.manifold import TSNE
from typing import Dict, List, Tuple, Any
class RelVVisualizer:
    """Visualization tools for the RelV database"""

    def __init__(self, relv_db: 'RelV'):
        """
        Initialize visualizer

        Args:
            relv_db: RelV database instance
        """
        self.relv_db = relv_db

    def plot_graph(self, figsize: Tuple[int, int] = (12, 8),
                   node_size: int = 200,
                   node_color: str = 'skyblue',
                   edge_threshold: float = None):
        """
        Visualize the graph structure of the RelV database

        Args:
            figsize: Figure size (width, height)
            node_size: Size of nodes in the plot
            node_color: Color of nodes
            edge_threshold: Minimum similarity to display an edge (defaults to RelV threshold)
        """
        plt.figure(figsize=figsize)

        # Create a copy of the graph for visualization
        if edge_threshold is not None:
            # Create a new graph with filtered edges
            viz_graph = nx.Graph()
            viz_graph.add_nodes_from(self.relv_db.graph.nodes())

            # Only include edges above threshold
            for u, v, attrs in self.relv_db.graph.edges(data=True):
                if attrs.get('weight', 0) >= edge_threshold:
                    viz_graph.add_edge(u, v, **attrs)
        else:
            viz_graph = self.relv_db.graph

        # Use spring layout for visualization
        pos = nx.spring_layout(viz_graph, seed=42)

        # Draw the graph
        nx.draw_networkx(
            viz_graph,
            pos,
            with_labels=False,
            node_size=node_size,
            node_color=node_color,
            alpha=0.8,
            width=[viz_graph[u][v].get('weight', 1.0) * 2 for u, v in viz_graph.edges()]
        )

        # Add node labels with truncated content
        labels = {}
        for node_id in viz_graph.nodes():
            content = self.relv_db.get_node_content(node_id)
            truncated = content[:20] + "..." if len(content) > 20 else content
            labels[node_id] = truncated

        nx.draw_networkx_labels(viz_graph, pos, labels=labels, font_size=8)

        plt.title("RelV Database Graph Structure")
        plt.axis('off')
        plt.tight_layout()
        plt.show()

    def plot_embeddings_2d(self, figsize: Tuple[int, int] = (10, 8),
                          node_size: int = 100,
                          random_state: int = 42,
                          annotate: bool = True):
        """
        Visualize node embeddings in 2D space using t-SNE

        Args:
            figsize: Figure size (width, height)
            node_size: Size of nodes in the plot
            random_state: Random seed for t-SNE
            annotate: Whether to annotate points with content snippets
        """
        if not self.relv_db.nodes:
            print("No nodes in the database to visualize")
            return

        # Collect all embeddings
        embeddings = []
        node_ids = []

        for node_id, node in self.relv_db.nodes.items():
            embeddings.append(node.embedding)
            node_ids.append(node_id)

        # Convert to numpy array
        embeddings_array = np.array(embeddings)

        # Apply t-SNE for dimensionality reduction
        tsne = TSNE(n_components=2, random_state=random_state, perplexity=min(30, len(embeddings)-1))
        reduced_embeddings = tsne.fit_transform(embeddings_array)

        # Plot
        plt.figure(figsize=figsize)
        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=node_size, alpha=0.6)

        if annotate:
            # Add annotations with text snippets
            for i, node_id in enumerate(node_ids):
                content = self.relv_db.get_node_content(node_id)
                snippet = content[:15] + "..." if len(content) > 15 else content
                plt.annotate(snippet, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]),
                           fontsize=8, alpha=0.7)

        plt.title("2D Visualization of Node Embeddings using t-SNE")
        plt.xlabel("Dimension 1")
        plt.ylabel("Dimension 2")
        plt.tight_layout()
        plt.show()

    def plot_adjacency_matrix(self, figsize: Tuple[int, int] = (10, 8),
                             cmap: str = 'viridis'):
        """
        Visualize the adjacency matrix

        Args:
            figsize: Figure size (width, height)
            cmap: Colormap to use
        """
        plt.figure(figsize=figsize)
        plt.imshow(self.relv_db.adjacency_matrix, cmap=cmap)
        plt.colorbar(label='Similarity')
        plt.title("RelV Adjacency Matrix")
        plt.xlabel("Node Index")
        plt.ylabel("Node Index")
        plt.tight_layout()
        plt.show()

    def community_analysis(self, resolution: float = 1.0):
        """
        Analyze and visualize communities in the graph

        Args:
            resolution: Resolution parameter for community detection
        """
        try:
            # Detect communities using Louvain method
            from community import community_louvain

            partition = community_louvain.best_partition(self.relv_db.graph, resolution=resolution)

            # Count communities
            communities = {}
            for node, community_id in partition.items():
                if community_id not in communities:
                    communities[community_id] = []
                communities[community_id].append(node)

            print(f"Detected {len(communities)} communities")

            # Visualize with community colors
            plt.figure(figsize=(12, 8))
            pos = nx.spring_layout(self.relv_db.graph, seed=42)

            # Draw nodes colored by community
            nx.draw_networkx_nodes(
                self.relv_db.graph,
                pos,
                node_size=200,
                cmap=plt.cm.rainbow,
                node_color=list(partition.values())
            )

            # Draw edges
            nx.draw_networkx_edges(
                self.relv_db.graph,
                pos,
                alpha=0.5
            )

            plt.title("Community Detection in RelV Database")
            plt.axis('off')
            plt.tight_layout()
            plt.show()

            # Print summary of communities
            for comm_id, nodes in communities.items():
                print(f"Community {comm_id}: {len(nodes)} nodes")
                if len(nodes) > 0:
                    sample_node = nodes[0]
                    print(f"  Sample content: {self.relv_db.get_node_content(sample_node)[:200]}...")

        except ImportError:
            print("Community detection requires python-louvain package.")
            print("Install it with: pip install python-louvain")

# After setting up your RelV database:
visualizer = RelVVisualizer(relv_db)
visualizer.plot_graph()
visualizer.plot_embeddings_2d()
visualizer.plot_adjacency_matrix()
visualizer.community_analysis()

import numpy as np
from typing import Dict, List, Tuple, Optional, Set, Union, Callable
import heapq
from collections import defaultdict
import time

class RelVOptimizer:
    """
    Advanced optimization tools for RelV database to handle
    small context windows and large-scale information retrieval
    """

    def __init__(self, relv_db: 'RelV', context_manager: 'RelVContextManager'):
        """
        Initialize the optimizer

        Args:
            relv_db: RelV database instance
            context_manager: RelV context manager instance
        """
        self.relv_db = relv_db
        self.context_manager = context_manager

    def find_information_bottlenecks(self) -> List[str]:
        """
        Identify nodes that act as information bottlenecks in the graph
        These are critical nodes that connect different subclusters

        Returns:
            List of node IDs that serve as bottlenecks
        """
        # Calculate betweenness centrality
        import networkx as nx
        betweenness = nx.betweenness_centrality(self.relv_db.graph)

        # Sort nodes by betweenness
        sorted_nodes = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)

        # Return top 10% or at least 3 nodes, whichever is greater
        count = max(3, int(len(self.relv_db.nodes) * 0.1))
        return [node_id for node_id, _ in sorted_nodes[:count]]

    def create_summary_nodes(self, community_threshold: float = 0.5) -> Dict[str, str]:
        """
        Create summary nodes for dense communities to reduce context size

        Args:
            community_threshold: Threshold for community detection

        Returns:
            Dictionary mapping summary node IDs to lists of summarized node IDs
        """
        # First detect communities
        import networkx as nx
        from community import community_louvain

        # Detect communities
        partition = community_louvain.best_partition(self.relv_db.graph)

        # Group nodes by community
        communities = defaultdict(list)
        for node_id, community_id in partition.items():
            communities[community_id].append(node_id)

        # Only create summaries for communities with more than 3 nodes
        summaries = {}

        for community_id, node_ids in communities.items():
            if len(node_ids) < 3:
                continue

            # Collect content from all nodes in the community
            community_content = []
            for node_id in node_ids:
                community_content.append(self.relv_db.get_node_content(node_id))

            # Join content into a single text
            all_content = "\n\n".join(community_content)

            # Create a summary using the LLM
            summary_prompt = f"""Summarize the following information in a concise manner,
            preserving key facts and relationships:

            {all_content[:2000]}  # Limit to avoid context overflow

            Provide a short summary that captures the essential information.
            """

            response = self.context_manager.llm(summary_prompt, max_tokens=200, temperature=0.1)
            summary = response["choices"][0]["text"].strip()

            # Create a new node with the summary
            embedding = self.context_manager.embedding_model.encode(summary)

            summary_node_id = self.relv_db.add_node(
                content=summary,
                embedding=embedding,
                metadata={
                    "type": "summary",
                    "summarized_nodes": node_ids,
                    "community_id": community_id
                }
            )

            # Add to results
            summaries[summary_node_id] = node_ids

        return summaries

    def create_hierarchical_index(self, levels: int = 2) -> Dict:
        """
        Create a hierarchical index for the database to enable efficient navigation

        Args:
            levels: Number of hierarchical levels

        Returns:
            Dictionary representing the hierarchical structure
        """
        # Use clustering to create hierarchy
        from sklearn.cluster import AgglomerativeClustering

        # Collect all embeddings
        embeddings = []
        node_ids = []

        for node_id, node in self.relv_db.nodes.items():
            embeddings.append(node.embedding)
            node_ids.append(node_id)

        embeddings_array = np.array(embeddings)

        # Create the hierarchical structure
        hierarchy = {}

        # Bottom level is just the original nodes
        hierarchy[levels] = {i: [node_id] for i, node_id in enumerate(node_ids)}

        # Create higher levels through clustering
        for level in range(levels-1, 0, -1):
            # Number of clusters at this level
            n_clusters = max(1, len(node_ids) // (5 ** (levels - level)))

            # Apply clustering
            clustering = AgglomerativeClustering(
                n_clusters=min(n_clusters, len(node_ids)),
                metric='euclidean',
                linkage='ward'
            )

            cluster_labels = clustering.fit_predict(embeddings_array)

            # Group nodes by cluster
            clusters = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                clusters[label].extend(hierarchy[level+1][i])

            # Store in hierarchy
            hierarchy[level] = clusters

        # Create summary nodes for each cluster
        summaries = {}

        for level in range(1, levels):
            level_summaries = {}

            for cluster_id, cluster_node_ids in hierarchy[level].items():
                # Get content from all nodes in cluster
                cluster_content = []
                for node_id in cluster_node_ids:
                    if node_id in self.relv_db.nodes:  # Check if node exists
                        cluster_content.append(self.relv_db.get_node_content(node_id))

                if not cluster_content:
                    continue

                # Create summary
                all_content = "\n\n".join(cluster_content[:5])  # Limit to top 5 to avoid overflow

                summary_prompt = f"""Create a concise summary of the following related information:

                {all_content[:1500]}

                Provide a brief summary that captures the main themes and key points.
                """

                response = self.context_manager.llm(summary_prompt, max_tokens=150, temperature=0.1)
                summary = response["choices"][0]["text"].strip()

                # Create embedding and add to database
                embedding = self.context_manager.embedding_model.encode(summary)

                summary_node_id = self.relv_db.add_node(
                    content=summary,
                    embedding=embedding,
                    metadata={
                        "type": "hierarchy_summary",
                        "level": level,
                        "cluster_id": cluster_id,
                        "summarized_nodes": cluster_node_ids
                    }
                )

                level_summaries[cluster_id] = summary_node_id

            summaries[level] = level_summaries

        # Add summaries to hierarchy information
        hierarchy["summaries"] = summaries

        return hierarchy

    def optimize_query_path(self, query: str, max_context_size: int = 1024) -> List[str]:
        """
        Find optimal path through the knowledge graph for a specific query
        to fit within context window constraints

        Args:
            query: Query string
            max_context_size: Maximum context size in tokens

        Returns:
            List of node IDs forming the optimal path
        """
        # Get query embedding
        query_embedding = self.context_manager.embedding_model.encode(query)

        # Find starting point - most relevant node
        relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=3)
        if not relevant_nodes:
            return []

        start_node_id = relevant_nodes[0][0]

        # Use A* search to find optimal path
        # The goal is to maximize relevance while staying within context limits

        # Priority queue for A* search
        # Format: (priority, path_length, current_node, path, visited)
        queue = [(0, 0, start_node_id, [start_node_id], {start_node_id})]

        best_path = [start_node_id]
        best_score = self._calculate_path_relevance(best_path, query_embedding)

        max_iterations = 100  # Limit search iterations
        iterations = 0

        while queue and iterations < max_iterations:
            iterations += 1

            # Get highest priority path
            _, path_length, current, path, visited = heapq.heappop(queue)

            # Check if current path is better than best found so far
            current_score = self._calculate_path_relevance(path, query_embedding)
            current_size = self._estimate_context_size(path)

            if current_score > best_score and current_size <= max_context_size:
                best_path = path.copy()
                best_score = current_score

            # Check neighbors for potential expansions
            for neighbor in self.relv_db.graph.neighbors(current):
                if neighbor not in visited:
                    # Create new path including this neighbor
                    new_path = path + [neighbor]
                    new_visited = visited.union({neighbor})
                    new_size = self._estimate_context_size(new_path)

                    # Only consider if within context limits
                    if new_size <= max_context_size:
                        # Calculate path score (negative for min-heap)
                        neighbor_relevance = self._calculate_node_relevance(neighbor, query_embedding)
                        path_relevance = self._calculate_path_relevance(new_path, query_embedding)

                        # Heuristic: prioritize relevance and path coherence
                        priority = -1 * (path_relevance + 0.5 * neighbor_relevance)

                        heapq.heappush(
                            queue,
                            (priority, path_length + 1, neighbor, new_path, new_visited)
                        )

        return best_path

    def _calculate_node_relevance(self, node_id: str, query_embedding: np.ndarray) -> float:
        """Calculate relevance of a node to query"""
        node = self.relv_db.get_node(node_id)
        return self.relv_db._calculate_similarity(node.embedding, query_embedding)

    def _calculate_path_relevance(self, path: List[str], query_embedding: np.ndarray) -> float:
        """Calculate overall relevance of a path to query"""
        if not path:
            return 0.0

        # Weighted sum of individual node relevances
        relevances = [self._calculate_node_relevance(node_id, query_embedding) for node_id in path]

        # Recent nodes have higher weight
        weights = np.linspace(0.5, 1.0, len(relevances))

        return np.average(relevances, weights=weights)

    def _estimate_context_size(self, node_ids: List[str]) -> int:
        """Estimate token size of nodes"""
        # Rough estimate: 1 token per 4 characters
        total_chars = 0
        for node_id in node_ids:
            content = self.relv_db.get_node_content(node_id)
            total_chars += len(content)

        return total_chars // 4

    def create_information_pathway(self, start_query: str, end_query: str) -> List[str]:
        """
        Create an information pathway connecting two queries

        Args:
            start_query: Starting point query
            end_query: Destination query

        Returns:
            List of node IDs forming the pathway
        """
        # Get embeddings for both queries
        start_embedding = self.context_manager.embedding_model.encode(start_query)
        end_embedding = self.context_manager.embedding_model.encode(end_query)

        # Find most relevant nodes for each query
        start_nodes = self.relv_db.query_by_embedding(start_embedding, k=2)
        end_nodes = self.relv_db.query_by_embedding(end_embedding, k=2)

        if not start_nodes or not end_nodes:
            return []

        start_node_id = start_nodes[0][0]
        end_node_id = end_nodes[0][0]

        # Find shortest path in graph
        try:
            import networkx as nx
            path = nx.shortest_path(self.relv_db.graph, start_node_id, end_node_id)
            return path
        except (nx.NetworkXNoPath, nx.NodeNotFound):
            # No direct path, find intermediate nodes
            # Create a temporary connection to enable search
            bridge_embedding = (start_embedding + end_embedding) / 2

            # Find nodes similar to the bridge
            bridge_nodes = self.relv_db.query_by_embedding(bridge_embedding, k=3)

            if not bridge_nodes:
                return []

            # Try to connect through bridge nodes
            for bridge_id, _ in bridge_nodes:
                # Try path from start to bridge
                try:
                    path1 = nx.shortest_path(self.relv_db.graph, start_node_id, bridge_id)
                    # Try path from bridge to end
                    path2 = nx.shortest_path(self.relv_db.graph, bridge_id, end_node_id)
                    # Combine paths (remove duplicate bridge node)
                    return path1 + path2[1:]
                except (nx.NetworkXNoPath, nx.NodeNotFound):
                    continue

            # If all else fails, return disconnected important nodes
            return [start_node_id, end_node_id]

class RelVEvaluator:
    """Evaluation framework for RelV database performance"""

    def __init__(self, relv_db: 'RelV', context_manager: 'RelVContextManager'):
        """
        Initialize the evaluator

        Args:
            relv_db: RelV database instance
            context_manager: RelV context manager instance
        """
        self.relv_db = relv_db
        self.context_manager = context_manager
        self.results = {}

    def benchmark_retrieval_speed(self, query_count: int = 20) -> Dict:
        """
        Benchmark the speed of retrieval operations

        Args:
            query_count: Number of random queries to test

        Returns:
            Dictionary with benchmark results
        """
        # Generate random query embeddings
        np.random.seed(42)
        query_embeddings = np.random.rand(query_count, self.relv_db.embedding_dim)

        # Normalize embeddings to unit length
        for i in range(query_count):
            query_embeddings[i] = query_embeddings[i] / np.linalg.norm(query_embeddings[i])

        # Test regular retrieval
        regular_times = []
        for i in range(query_count):
            start_time = time.time()
            _ = self.relv_db.query_by_embedding(query_embeddings[i], k=5)
            end_time = time.time()
            regular_times.append(end_time - start_time)

        avg_regular_time = np.mean(regular_times)
        print(f"Average retrieval time: {avg_regular_time:.5f} seconds")

        # Test retrieval with path optimization if at least 10 nodes
        if len(self.relv_db.nodes) >= 10:
            optimizer = RelVOptimizer(self.relv_db, self.context_manager)

            optimized_times = []
            for i in range(query_count):
                start_time = time.time()
                _ = optimizer.optimize_query_path("Test query " + str(i))
                end_time = time.time()
                optimized_times.append(end_time - start_time)

            avg_optimized_time = np.mean(optimized_times)
            print(f"Average optimized path time: {avg_optimized_time:.5f} seconds")
        else:
            avg_optimized_time = None

        results = {
            "average_retrieval_time": avg_regular_time,
            "average_optimized_time": avg_optimized_time,
            "node_count": len(self.relv_db.nodes),
            "edge_count": self.relv_db.graph.number_of_edges()
        }

        self.results["retrieval_speed"] = results
        return results

    def evaluate_contextual_relevance(self, test_queries: List[str],
                                     ground_truth: Dict[str, List[str]] = None) -> Dict:
        """
        Evaluate the relevance of retrieved context

        Args:
            test_queries: List of test queries
            ground_truth: Optional mapping of query to relevant node IDs

        Returns:
            Dictionary with evaluation results
        """
        results = {
            "query_results": {},
            "overall_stats": {}
        }

        similarities = []

        for query in test_queries:
            # Get query embedding
            query_embedding = self.context_manager.embedding_model.encode(query)

            # Get relevant nodes
            relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=5)

            # Calculate average similarity
            avg_similarity = np.mean([sim for _, sim in relevant_nodes])
            similarities.append(avg_similarity)

            # Store individual results
            results["query_results"][query] = {
                "retrieved_nodes": relevant_nodes,
                "average_similarity": avg_similarity
            }

            # Check against ground truth if provided
            if ground_truth and query in ground_truth:
                true_relevant = set(ground_truth[query])
                retrieved = set([node_id for node_id, _ in relevant_nodes])

                # Calculate metrics
                intersection = true_relevant.intersection(retrieved)
                precision = len(intersection) / len(retrieved) if retrieved else 0
                recall = len(intersection) / len(true_relevant) if true_relevant else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

                results["query_results"][query].update({
                    "precision": precision,
                    "recall": recall,
                    "f1": f1
                })

        # Calculate overall stats
        results["overall_stats"]["average_similarity"] = np.mean(similarities)

        if ground_truth:
            overall_precision = np.mean([r.get("precision", 0) for r in results["query_results"].values()])
            overall_recall = np.mean([r.get("recall", 0) for r in results["query_results"].values()])
            overall_f1 = np.mean([r.get("f1", 0) for r in results["query_results"].values()])

            results["overall_stats"].update({
                "precision": overall_precision,
                "recall": overall_recall,
                "f1": overall_f1
            })

        self.results["contextual_relevance"] = results
        return results

    def evaluate_slm_performance(self, test_queries: List[str],
                               evaluate_fn: Callable = None) -> Dict:
        """
        Evaluate performance improvement for small language models

        Args:
            test_queries: List of test queries
            evaluate_fn: Optional function to evaluate response quality

        Returns:
            Dictionary with evaluation results
        """
        results = {
            "queries": {},
            "overall": {}
        }

        # Run queries with and without RelV context enhancement
        for query in tqdm(test_queries):
            # Without RelV context (just direct query)
            start_time = time.time()
            direct_response = self.context_manager.llm(query, max_tokens=256, temperature=0.1)
            direct_time = time.time() - start_time
            direct_text = direct_response["choices"][0]["text"]

            # With RelV context enhancement
            start_time = time.time()
            enhanced_response = self.context_manager.query(query, k=3)
            enhanced_time = time.time() - start_time

            # Store results
            results["queries"][query] = {
                "direct_response": direct_text,
                "enhanced_response": enhanced_response,
                "direct_time": direct_time,
                "enhanced_time": enhanced_time
            }

            # Use evaluation function if provided
            if evaluate_fn:
                direct_score = evaluate_fn(query, direct_text)
                enhanced_score = evaluate_fn(query, enhanced_response)

                results["queries"][query]["direct_score"] = direct_score
                results["queries"][query]["enhanced_score"] = enhanced_score
                results["queries"][query]["improvement"] = enhanced_score - direct_score

        # Calculate overall stats
        direct_times = [r["direct_time"] for r in results["queries"].values()]
        enhanced_times = [r["enhanced_time"] for r in results["queries"].values()]

        results["overall"]["avg_direct_time"] = np.mean(direct_times)
        results["overall"]["avg_enhanced_time"] = np.mean(enhanced_times)
        results["overall"]["time_overhead"] = np.mean(enhanced_times) - np.mean(direct_times)

        if evaluate_fn:
            improvements = [r["improvement"] for r in results["queries"].values()]
            results["overall"]["avg_improvement"] = np.mean(improvements)
            results["overall"]["improvement_stddev"] = np.std(improvements)

        self.results["slm_performance"] = results
        return results

    def evaluate_memory_efficiency(self, node_counts: List[int]) -> Dict:
        """
        Evaluate memory usage efficiency at different scales

        Args:
            node_counts: List of node counts to test

        Returns:
            Dictionary with memory evaluation results
        """
        import os
        import psutil
        import gc

        results = {
            "memory_usage": [],
            "adjacency_size": [],
            "graph_size": []
        }

        process = psutil.Process(os.getpid())

        # Test at different scales
        current_nodes = len(self.relv_db.nodes)

        for target_count in node_counts:
            if target_count <= current_nodes:
                # Measure current memory usage
                gc.collect()
                memory_usage = process.memory_info().rss / (1024 * 1024)  # MB

                # Estimate adjacency matrix size
                adj_size = self.relv_db.adjacency_matrix.size * self.relv_db.adjacency_matrix.itemsize
                adj_size_mb = adj_size / (1024 * 1024)

                results["memory_usage"].append(memory_usage)
                results["adjacency_size"].append(adj_size_mb)
                results["graph_size"].append(self.relv_db.graph.number_of_edges())

            else:
                print(f"Skipping node count {target_count}, exceeds current DB size")

        # Plot results
        if len(results["memory_usage"]) > 1:
            fig, axs = plt.subplots(1, 2, figsize=(14, 6))

            # Memory usage plot
            axs[0].plot(node_counts[:len(results["memory_usage"])], results["memory_usage"],
                      marker='o', label='Total Memory')
            axs[0].plot(node_counts[:len(results["adjacency_size"])], results["adjacency_size"],
                      marker='s', label='Adjacency Matrix')

            axs[0].set_xlabel('Number of Nodes')
            axs[0].set_ylabel('Memory Usage (MB)')
            axs[0].set_title('Memory Scaling')
            axs[0].legend()

            # Graph edges plot
            axs[1].plot(node_counts[:len(results["graph_size"])], results["graph_size"],
                      marker='o', color='green')
            axs[1].set_xlabel('Number of Nodes')
            axs[1].set_ylabel('Number of Edges')
            axs[1].set_title('Graph Size Scaling')

            plt.tight_layout()
            plt.show()

        self.results["memory_efficiency"] = results
        return results

    def plot_results(self):
        """Plot all evaluation results"""
        if not self.results:
            print("No evaluation results available")
            return

        # Create a figure with subplots based on available results
        num_plots = len(self.results)
        fig, axs = plt.subplots(num_plots, 1, figsize=(10, 5 * num_plots))

        if num_plots == 1:
            axs = [axs]  # Make iterable if only one subplot

        plot_idx = 0

        # Plot retrieval speed results
        if "retrieval_speed" in self.results:
            retrieval = self.results["retrieval_speed"]

            times = [retrieval["average_retrieval_time"]]
            labels = ["Standard"]

            if retrieval["average_optimized_time"]:
                times.append(retrieval["average_optimized_time"])
                labels.append("Optimized")

            axs[plot_idx].bar(labels, times, color=['blue', 'orange'])
            axs[plot_idx].set_ylabel('Time (seconds)')
            axs[plot_idx].set_title('Retrieval Speed Comparison')

            plot_idx += 1

        # Plot contextual relevance
        if "contextual_relevance" in self.results:
            relevance = self.results["contextual_relevance"]

            if "overall_stats" in relevance:
                stats = relevance["overall_stats"]

                if all(m in stats for m in ["precision", "recall", "f1"]):
                    metrics = ["precision", "recall", "f1"]
                    values = [stats[m] for m in metrics]

                    axs[plot_idx].bar(metrics, values, color=['green', 'red', 'purple'])
                    axs[plot_idx].set_ylabel('Score')
                    axs[plot_idx].set_title('Relevance Metrics')
                    axs[plot_idx].set_ylim(0, 1)

                    plot_idx += 1

        # Plot SLM performance improvement
        if "slm_performance" in self.results:
            perf = self.results["slm_performance"]

            if "overall" in perf and "avg_improvement" in perf["overall"]:
                improvement = perf["overall"]["avg_improvement"]

                axs[plot_idx].bar(["Performance Improvement"], [improvement],
                               color='blue' if improvement >= 0 else 'red')
                axs[plot_idx].set_title('Average Performance Improvement')
                axs[plot_idx].set_ylabel('Score Difference')

                plot_idx += 1

        plt.tight_layout()
        plt.show()

!pip install pymupdf pdf2image pytesseract pillow

!pip install opencv-python

import fitz  # PyMuPDF
from pdf2image import convert_from_path
import pytesseract
from PIL import Image
import os

def is_text_pdf(pdf_path, sample_pages=3):
    """
    Checks if a PDF contains embedded text by scanning the first few pages.

    Args:
        pdf_path (str): Path to the PDF file.
        sample_pages (int): Number of pages to check for embedded text.

    Returns:
        bool: True if embedded text is detected, False otherwise.
    """
    doc = fitz.open(pdf_path)  # Open PDF
    for page_num in range(min(sample_pages, len(doc))):  # Check first few pages
        text = doc[page_num].get_text().strip()
        if text:  # If text is found, return True
            return True
    return False  # No text found, assume it's an image-based PDF

import cv2
import numpy as np

def preprocess_image(image):
    """
    Applies preprocessing techniques to improve OCR accuracy.

    Args:
        image (PIL.Image): Input image.

    Returns:
        preprocessed_img (OpenCV Image): Processed image.
    """
    # Convert PIL Image to OpenCV format
    img = np.array(image)
    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert to grayscale

    # Apply noise reduction (Gaussian Blur)
    img = cv2.GaussianBlur(img, (5, 5), 0)

    # Apply adaptive thresholding for better contrast
    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, 11, 2)

    # Deskew image
    coords = np.column_stack(np.where(img > 0))  # Find text coordinates
    angle = cv2.minAreaRect(coords)[-1]  # Get the angle
    if angle < -45:
        angle = -(90 + angle)
    else:
        angle = -angle

    (h, w) = img.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)  # Compute rotation matrix
    img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC,
                         borderMode=cv2.BORDER_REPLICATE)  # Rotate image

    return img

def compute_noise_score(image):
    """
    Computes a noise score based on image sharpness and contrast.

    Args:
        image (PIL.Image): Input image.

    Returns:
        float: Noise score (higher = less noisy, lower = more noise).
    """
    # Convert PIL Image to OpenCV format (grayscale)
    img = np.array(image.convert('L'))

    # 1. Sharpness measurement using Laplacian variance
    laplacian_var = cv2.Laplacian(img, cv2.CV_64F).var()

    # 2. Contrast measurement using RMS contrast
    rms_contrast = np.std(img)

    # Combine the two scores (adjust weighting as needed)
    noise_score = (laplacian_var * 0.7) + (rms_contrast * 0.3)

    return noise_score
def extract_text_pymupdf(pdf_path):
    """
    Extracts text from a text-based PDF using PyMuPDF.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        str: Extracted text.
    """
    doc = fitz.open(pdf_path)
    text = "\n".join([page.get_text() for page in doc])  # Extract text from all pages
    return text

def extract_text_ocr(pdf_path, noise_threshold=100):
    """
    Extracts text from an image-based PDF using OCR with optional preprocessing.

    Args:
        pdf_path (str): Path to the PDF file.
        noise_threshold (float): Minimum score required to skip preprocessing.

    Returns:
        str: Extracted text.
    """
    images = convert_from_path(pdf_path)  # Convert PDF pages to images
    extracted_text = ""

    for image in images:
        noise_score = compute_noise_score(image)
        print(f"Noise Score: {noise_score}")

        if noise_score < noise_threshold:
            print("Applying preprocessing due to low noise score...")
            processed_img = preprocess_image(image)  # Apply preprocessing
        else:
            print("Skipping preprocessing...")
            processed_img = np.array(image.convert('L'))  # Use original image

        text = pytesseract.image_to_string(processed_img)  # Perform OCR
        extracted_text += text + "\n"

    return extracted_text

def process_pdf(pdf_path):
    """
    Determines whether a PDF contains embedded text or is image-based,
    then extracts text accordingly.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        str: Extracted text.
    """
    if is_text_pdf(pdf_path):
        print("PDF contains embedded text. Using PyMuPDF for extraction...")
        return extract_text_pymupdf(pdf_path)
    else:
        print("PDF appears to be image-based. Using OCR for extraction...")
        return extract_text_ocr(pdf_path)

!apt-get update
!apt-get install poppler-utils

!apt-get install tesseract-ocr
!apt-get install libtesseract-dev

# Assuming you have the extracted text in variables named
"""Scanned PDF Example"""
pdf_path = "/content/Personal Fitness and Wellness Contract (1).pdf"  # Replace with actual PDF file path
extracted_text_pdf = process_pdf(pdf_path)
print(extracted_text_pdf[:1000])  # Print first 1000 characters to check output
# Add PDF text
pdf_nodes = context_manager.add_document(extracted_text_pdf, metadata={"source": "PDF"})
print(f"Added {len(pdf_nodes)} nodes from PDF text")

print("Finished adding extracted texts to RelV database.")

visualizer = RelVVisualizer(relv_db)
visualizer.plot_graph()
visualizer.plot_embeddings_2d()
visualizer.plot_adjacency_matrix()
visualizer.community_analysis()

def print_relv_info(relv_db):
    """Prints information about the RelV database, including nodes, adjacency matrix, and graph."""
    print("Nodes in the RelV database:")
    for node_id, node in relv_db.nodes.items():
        print(f"  Node ID: {node_id}")
        print(f"    Content: {node.content}")
        print(f"    Metadata: {node.metadata}")
        print(f"    Embedding shape: {node.embedding.shape}")
        print("-" * 20)

    print("\nAdjacency Matrix:")
    print(relv_db.adjacency_matrix)

    print("\nGraph Information:")
    print(f"  Number of nodes: {relv_db.graph.number_of_nodes()}")
    print(f"  Number of edges: {relv_db.graph.number_of_edges()}")
    print(f"  Edges:")
    for u, v, data in relv_db.graph.edges(data=True):
        print(f"    {u} -- {v} (weight: {data['weight']})")

# Call the function to print the information
print_relv_info(relv_db)