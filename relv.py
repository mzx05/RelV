# -*- coding: utf-8 -*-
"""RelV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VRqH0pLEFB2qEi7GfdRfHSWK4IGLvEZO
"""

# Install necessary libraries with CUDA support
!pip install -q torch

import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():   print(f"GPU: {torch.cuda.get_device_name(0)}")

# Check CUDA version first
!nvcc --version

# Install llama-cpp-python with CUDA 12.x support
!pip install --no-cache-dir llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123

# Install LlamaIndex
!pip install llama-index

from llama_cpp import Llama
import os

# Define model path
model_path = "/content/mistral.gguf"

# Download Mistral model if not already present
if not os.path.exists(model_path):
    !wget "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" -O {model_path}
print(f"Model downloaded to {model_path}")

# Verify model file exists
if os.path.exists(model_path):   print(f"Model file exists. Size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
else:   print("Model file not found!")

# Load the model with GPU acceleration
try:
  llm = Llama(model_path=model_path,     n_gpu_layers=1,  n_ctx=2048, verbose=True)
  print("Model loaded successfully!")

except Exception as e:   print(f"Error loading model: {e}")

import numpy as np
import torch
from typing import Dict, List, Tuple, Optional, Union
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from dataclasses import dataclass
import uuid

@dataclass
class Node:
    """Represents a node in the RelV database."""
    id: str
    content: str
    embedding: np.ndarray
    metadata: Dict = None

    def __init__(self, content: str, embedding: np.ndarray, node_id: str = None, metadata: Dict = None):
        self.id = node_id if node_id else str(uuid.uuid4())
        self.content = content
        self.embedding = embedding
        self.metadata = metadata if metadata else {}

class RelV:
    """
    Dynamic Relational Vector Database with Adjacency Matrix Representation
    Optimized for SLMs with small context windows and external memory for LLMs
    """

    def __init__(self, embedding_dim: int = 768, similarity_threshold: float = 0.6):
        """
        Initialize a new RelV database

        Args:
            embedding_dim: Dimension of the vector embeddings
            similarity_threshold: Threshold for considering nodes as related
        """
        self.nodes: Dict[str, Node] = {}
        self.adjacency_matrix = np.zeros((0, 0), dtype=float)
        self.embedding_dim = embedding_dim
        self.similarity_threshold = similarity_threshold
        self.graph = nx.Graph()

    def add_node(self, content: str, embedding: np.ndarray, metadata: Dict = None) -> str:
        """
        Add a new node to the database

        Args:
            content: Text content of the node
            embedding: Vector embedding of the content
            metadata: Additional metadata for the node

        Returns:
            ID of the newly created node
        """
        # Create a new node
        node = Node(content=content, embedding=embedding, metadata=metadata)
        node_id = node.id

        # Add node to the collection
        self.nodes[node_id] = node

        # Update adjacency matrix
        if len(self.nodes) == 1:
            # First node, initialize adjacency matrix
            self.adjacency_matrix = np.zeros((1, 1), dtype=float)
        else:
            # Calculate similarities with existing nodes
            old_size = len(self.nodes) - 1
            new_matrix = np.zeros((old_size + 1, old_size + 1), dtype=float)
            new_matrix[:old_size, :old_size] = self.adjacency_matrix

            # Calculate similarities between the new node and all existing nodes
            for i, existing_id in enumerate(list(self.nodes.keys())[:-1]):
                existing_node = self.nodes[existing_id]
                sim = self._calculate_similarity(node.embedding, existing_node.embedding)
                new_matrix[i, old_size] = sim
                new_matrix[old_size, i] = sim

                # Update graph if nodes are connected
                if sim >= self.similarity_threshold:
                    self.graph.add_edge(node_id, existing_id, weight=sim)

            self.adjacency_matrix = new_matrix

        # Add node to graph
        self.graph.add_node(node_id)

        return node_id

    def _calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Calculate cosine similarity between two embeddings"""
        e1 = embedding1.reshape(1, -1)
        e2 = embedding2.reshape(1, -1)
        return cosine_similarity(e1, e2)[0][0]

    def get_related_nodes(self, node_id: str, k: int = 5, min_similarity: float = None) -> List[Tuple[str, float]]:
        """
        Find the k most related nodes to the given node

        Args:
            node_id: ID of the node to find relations for
            k: Number of related nodes to return
            min_similarity: Minimum similarity threshold (defaults to class threshold if None)

        Returns:
            List of tuples containing (node_id, similarity_score) sorted by similarity
        """
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")

        threshold = min_similarity if min_similarity is not None else self.similarity_threshold
        node_index = list(self.nodes.keys()).index(node_id)

        # Get similarities from adjacency matrix
        similarities = [(i, self.adjacency_matrix[node_index, i])
                       for i in range(len(self.nodes)) if i != node_index]

        # Filter by threshold and sort
        filtered_similarities = [(list(self.nodes.keys())[i], sim)
                                for i, sim in similarities if sim >= threshold]
        sorted_similarities = sorted(filtered_similarities, key=lambda x: x[1], reverse=True)

        return sorted_similarities[:k]

    def query_by_embedding(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[str, float]]:
        """
        Find the k most similar nodes to the given query embedding

        Args:
            query_embedding: Query vector
            k: Number of results to return

        Returns:
            List of tuples containing (node_id, similarity_score) sorted by similarity
        """
        similarities = []

        for node_id, node in self.nodes.items():
            sim = self._calculate_similarity(query_embedding, node.embedding)
            similarities.append((node_id, sim))

        # Sort by similarity (descending)
        sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)

        return sorted_similarities[:k]

    def get_node_content(self, node_id: str) -> str:
        """Get the content of a node by its ID"""
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")
        return self.nodes[node_id].content

    def get_node(self, node_id: str) -> Node:
        """Get a node by its ID"""
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")
        return self.nodes[node_id]

    def get_path_between_nodes(self, start_id: str, end_id: str) -> List[str]:
        """Find the shortest path between two nodes"""
        if start_id not in self.nodes or end_id not in self.nodes:
            raise ValueError("One or both node IDs not found")

        try:
            path = nx.shortest_path(self.graph, start_id, end_id)
            return path
        except nx.NetworkXNoPath:
            return []

    def remove_node(self, node_id: str) -> bool:
        """Remove a node from the database"""
        if node_id not in self.nodes:
            return False

        # Get index of node to remove
        node_index = list(self.nodes.keys()).index(node_id)

        # Remove node from collection
        del self.nodes[node_id]

        # Update adjacency matrix
        indices = list(range(len(self.adjacency_matrix)))
        indices.remove(node_index)
        self.adjacency_matrix = self.adjacency_matrix[np.ix_(indices, indices)]

        # Remove from graph
        self.graph.remove_node(node_id)

        return True

    def get_community(self, node_id: str, depth: int = 2) -> List[str]:
        """Get all nodes connected to the given node up to a certain depth"""
        if node_id not in self.nodes:
            raise ValueError(f"Node with ID {node_id} not found")

        # Use BFS to find nodes up to given depth
        visited = {node_id}
        queue = [(node_id, 0)]  # (node_id, depth)
        community = []

        while queue:
            current_id, current_depth = queue.pop(0)
            if current_depth > 0:  # Don't include the source node
                community.append(current_id)

            if current_depth < depth:
                for neighbor in self.graph.neighbors(current_id):
                    if neighbor not in visited:
                        visited.add(neighbor)
                        queue.append((neighbor, current_depth + 1))

        return community

    def get_stats(self) -> Dict:
        """Get statistics about the database"""
        return {
            "node_count": len(self.nodes),
            "edge_count": self.graph.number_of_edges(),
            "density": nx.density(self.graph),
            "connected_components": nx.number_connected_components(self.graph),
            "avg_clustering": nx.average_clustering(self.graph)
        }

import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer
import re
from llama_cpp import Llama

class RelVContextManager:
    """
    Manages context for LLMs using the RelV database for dynamic memory
    """

    def __init__(self,
                 relv_db: 'RelV',
                 llm: Any,
                 embedding_model: Optional[SentenceTransformer] = None,
                 chunk_size: int = 512,
                 overlap: int = 50,
                 max_context_size: int = 2048):
        """
        Initialize the RelV Context Manager

        Args:
            relv_db: RelV database instance
            llm: Language model instance (e.g., Llama model)
            embedding_model: Model to create embeddings (if None, must provide embeddings)
            chunk_size: Size of text chunks for processing
            overlap: Overlap between chunks
            max_context_size: Maximum context size for the LLM
        """
        self.relv_db = relv_db
        self.llm = llm
        self.embedding_model = embedding_model
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.max_context_size = max_context_size
        self.query_history = []

    def add_document(self, document: str, metadata: Dict = None) -> List[str]:
        """
        Add a document to the RelV database by chunking and embedding

        Args:
            document: Text document to add
            metadata: Additional metadata for the document

        Returns:
            List of node IDs created
        """
        if not document:
            return []

        chunks = self._chunk_text(document)
        node_ids = []

        for i, chunk in enumerate(chunks):
            # Create chunk-specific metadata
            chunk_metadata = {
                "chunk_index": i,
                "total_chunks": len(chunks),
                "source": "document"
            }

            # Merge with provided metadata
            if metadata:
                chunk_metadata.update(metadata)

            # Create embedding
            if self.embedding_model:
                embedding = self.embedding_model.encode(chunk)
            else:
                raise ValueError("Embedding model not provided. Cannot embed text.")

            # Add to database
            node_id = self.relv_db.add_node(
                content=chunk,
                embedding=embedding,
                metadata=chunk_metadata
            )

            node_ids.append(node_id)

        return node_ids

    def _chunk_text(self, text: str) -> List[str]:
        """Split text into chunks with overlap"""
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            # Find end of chunk
            end = min(start + self.chunk_size, len(text))

            # If not at the end of text, try to find a good breaking point
            if end < len(text):
                # Look for period, newline, etc. within the last 20% of the chunk
                last_part = text[end - int(0.2 * self.chunk_size):end]

                # Find the last occurrence of period, question mark, etc.
                matches = list(re.finditer(r'[.!?\n]', last_part))
                if matches:
                    # Get position relative to the entire text
                    break_pos = end - int(0.2 * self.chunk_size) + matches[-1].start() + 1
                    end = break_pos

            # Add chunk
            chunks.append(text[start:end])

            # Calculate next start position with overlap
            start = max(0, end - self.overlap)

        return chunks

    def query(self, query: str, k: int = 3) -> str:
        """
        Process a query using the RelV database and LLM

        Args:
            query: User query
            k: Number of related nodes to include in context

        Returns:
            Response from the LLM
        """
        # Store query in history
        self.query_history.append(query)

        # Get embedding for the query
        if self.embedding_model:
            query_embedding = self.embedding_model.encode(query)
        else:
            raise ValueError("Embedding model not provided. Cannot embed query.")

        # Find relevant nodes
        relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=k)

        # Build context from relevant nodes
        context = []
        total_length = 0

        for node_id, similarity in relevant_nodes:
            node_content = self.relv_db.get_node_content(node_id)

            # Check if adding this content would exceed max context size
            if total_length + len(node_content) + len(query) + 100 > self.max_context_size:
                break

            context.append(f"[Relevant Information (Similarity: {similarity:.2f})]\n{node_content}\n")
            total_length += len(node_content) + 100  # Add some buffer for formatting

        # Create prompt with context
        prompt = f"""The following information is relevant to the query:

{' '.join(context)}

Based on the above information, please answer the following query:
{query}
"""

        # Query the LLM
        response = self.llm(prompt, max_tokens=512, temperature=0.1)

        return response["choices"][0]["text"]

    def add_conversation_to_memory(self, query: str, response: str) -> str:
        """
        Add a conversation exchange to the RelV memory

        Args:
            query: User query
            response: LLM response

        Returns:
            Node ID of the stored conversation
        """
        conversation = f"User: {query}\nModel Response: {response}"

        if self.embedding_model:
            embedding = self.embedding_model.encode(conversation)
        else:
            raise ValueError("Embedding model not provided. Cannot embed conversation.")

        # Add to database
        node_id = self.relv_db.add_node(
            content=conversation,
            embedding=embedding,
            metadata={"type": "conversation"}
        )

        return node_id

    def get_dynamic_context(self, query: str, history_weight: float = 0.3, k_recent: int = 2, k_relevant: int = 3) -> List[str]:
        """
        Build a dynamic context using both relevance and recency

        Args:
            query: Current query
            history_weight: Weight given to conversation history vs relevance
            k_recent: Number of recent conversations to include
            k_relevant: Number of relevant nodes to include

        Returns:
            List of node contents to use as context
        """
        # Get embedding for the query
        if self.embedding_model:
            query_embedding = self.embedding_model.encode(query)
        else:
            raise ValueError("Embedding model not provided. Cannot embed query.")

        # Get relevant nodes
        relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=k_relevant)

        # Find recent conversation nodes
        conversation_nodes = {node_id: node for node_id, node in self.relv_db.nodes.items()
                             if node.metadata and node.metadata.get("type") == "conversation"}

        # Sort by recency (assuming node IDs are chronological)
        recent_nodes = list(conversation_nodes.items())[-k_recent:]

        # Combine information sources
        context_nodes = []

        # Add recent conversation nodes
        for node_id, node in recent_nodes:
            context_nodes.append((node_id, history_weight))

        # Add relevant nodes
        for node_id, similarity in relevant_nodes:
            # Skip if already included
            if node_id in [n[0] for n in context_nodes]:
                continue
            context_nodes.append((node_id, (1 - history_weight) * similarity))

        # Sort by combined weight
        context_nodes.sort(key=lambda x: x[1], reverse=True)

        # Get node contents
        context_contents = []
        total_length = 0

        for node_id, _ in context_nodes:
            content = self.relv_db.get_node_content(node_id)

            # Check if adding this would exceed max context size
            if total_length + len(content) + len(query) + 100 > self.max_context_size:
                break

            context_contents.append(content)
            total_length += len(content) + 20  # Add some buffer

        return context_contents

# Install additional required packages
!pip install -q sentence-transformers networkx scikit-learn

# Import necessary libraries
import torch
import numpy as np
from llama_cpp import Llama
import os
from sentence_transformers import SentenceTransformer

# Load the embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
print("Embedding model loaded successfully!")

# Initialize RelV database
relv_db = RelV(embedding_dim=384)  # MiniLM has 384 dimensions

# Initialize context manager
context_manager = RelVContextManager(
    relv_db=relv_db,
    llm=llm,
    embedding_model=embedding_model,
    chunk_size=512,
    overlap=50,
    max_context_size=2048
)

# Example: Add some documents to the database
documents = [
    """Retrieval-Augmented Generation (RAG) is a technique that enhances large language models by combining them with retrieval systems.
    These systems fetch relevant information from external knowledge sources to supplement the model's internal knowledge,
    providing more accurate and up-to-date responses.""",

    """The adjacency matrix is a square matrix used to represent a finite graph. The elements of the matrix indicate
    whether pairs of vertices are adjacent or not in the graph. For a simple graph with no self-loops,
    the adjacency matrix must have 0s on the diagonal.""",

    """Vector databases store data as high-dimensional vectors and support similarity search operations.
    They are optimized for machine learning applications, particularly those involving embeddings.
    Popular vector databases include Pinecone, Milvus, and FAISS.""",

    """Small Language Models (SLMs) are compact neural networks designed for natural language processing tasks.
    Despite their smaller size compared to large language models (LLMs), they can be effective for specific applications,
    especially when computational resources are limited."""
]

# Add documents to RelV
print("Adding documents to RelV database...")
node_ids = []
for i, doc in enumerate(documents):
    print(f"Adding document {i+1}/{len(documents)}...")
    doc_nodes = context_manager.add_document(doc, metadata={"source": f"document_{i}"})
    node_ids.extend(doc_nodes)

print(f"Added {len(node_ids)} nodes to the database")

# Display database statistics
print("\nRelV Database Statistics:")
stats = relv_db.get_stats()
for key, value in stats.items():
    print(f"{key}: {value}")

# Example query
query = "What is the relationship between RAG and vector databases?"
print(f"\nProcessing query: {query}")

response = context_manager.query(query, k=3)
print("\nResponse:")
print(response)

# Add conversation to memory
node_id = context_manager.add_conversation_to_memory(query, response)
print(f"Conversation added to memory with node ID: {node_id}")

# Try another query that benefits from previous context
follow_up_query = "How can small language models benefit from this approach?"
print(f"\nProcessing follow-up query: {follow_up_query}")

# Get dynamic context
context = context_manager.get_dynamic_context(follow_up_query)
print(f"Dynamic context contains {len(context)} nodes")

# Process the follow-up query
follow_up_response = context_manager.query(follow_up_query, k=4)
print("\nFollow-up Response:")
print(follow_up_response)

# Display related nodes for a specific node
print("\nFinding related nodes...")
if node_ids:
    related = relv_db.get_related_nodes(node_ids[0], k=2)
    print(f"Nodes related to {node_ids[0]}:")
    for rel_id, sim in related:
        print(f"Node: {rel_id}, Similarity: {sim:.4f}")
        print(f"Content: {relv_db.get_node_content(rel_id)[:600]}...")

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
from sklearn.manifold import TSNE
from typing import Dict, List, Tuple, Any

class RelVVisualizer:
    """Visualization tools for the RelV database"""

    def __init__(self, relv_db: 'RelV'):
        """
        Initialize visualizer

        Args:
            relv_db: RelV database instance
        """
        self.relv_db = relv_db

    def plot_graph(self, figsize: Tuple[int, int] = (12, 8),
                   node_size: int = 200,
                   node_color: str = 'skyblue',
                   edge_threshold: float = None):
        """
        Visualize the graph structure of the RelV database

        Args:
            figsize: Figure size (width, height)
            node_size: Size of nodes in the plot
            node_color: Color of nodes
            edge_threshold: Minimum similarity to display an edge (defaults to RelV threshold)
        """
        plt.figure(figsize=figsize)

        # Create a copy of the graph for visualization
        if edge_threshold is not None:
            # Create a new graph with filtered edges
            viz_graph = nx.Graph()
            viz_graph.add_nodes_from(self.relv_db.graph.nodes())

            # Only include edges above threshold
            for u, v, attrs in self.relv_db.graph.edges(data=True):
                if attrs.get('weight', 0) >= edge_threshold:
                    viz_graph.add_edge(u, v, **attrs)
        else:
            viz_graph = self.relv_db.graph

        # Use spring layout for visualization
        pos = nx.spring_layout(viz_graph, seed=42)

        # Draw the graph
        nx.draw_networkx(
            viz_graph,
            pos,
            with_labels=False,
            node_size=node_size,
            node_color=node_color,
            alpha=0.8,
            width=[viz_graph[u][v].get('weight', 1.0) * 2 for u, v in viz_graph.edges()]
        )

        # Add node labels with truncated content
        labels = {}
        for node_id in viz_graph.nodes():
            content = self.relv_db.get_node_content(node_id)
            truncated = content[:20] + "..." if len(content) > 20 else content
            labels[node_id] = truncated

        nx.draw_networkx_labels(viz_graph, pos, labels=labels, font_size=8)

        plt.title("RelV Database Graph Structure")
        plt.axis('off')
        plt.tight_layout()
        plt.show()

    def plot_embeddings_2d(self, figsize: Tuple[int, int] = (10, 8),
                          node_size: int = 100,
                          random_state: int = 42,
                          annotate: bool = True):
        """
        Visualize node embeddings in 2D space using t-SNE

        Args:
            figsize: Figure size (width, height)
            node_size: Size of nodes in the plot
            random_state: Random seed for t-SNE
            annotate: Whether to annotate points with content snippets
        """
        if not self.relv_db.nodes:
            print("No nodes in the database to visualize")
            return

        # Collect all embeddings
        embeddings = []
        node_ids = []

        for node_id, node in self.relv_db.nodes.items():
            embeddings.append(node.embedding)
            node_ids.append(node_id)

        # Convert to numpy array
        embeddings_array = np.array(embeddings)

        # Apply t-SNE for dimensionality reduction
        tsne = TSNE(n_components=2, random_state=random_state, perplexity=min(30, len(embeddings)-1))
        reduced_embeddings = tsne.fit_transform(embeddings_array)

        # Plot
        plt.figure(figsize=figsize)
        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=node_size, alpha=0.6)

        if annotate:
            # Add annotations with text snippets
            for i, node_id in enumerate(node_ids):
                content = self.relv_db.get_node_content(node_id)
                snippet = content[:15] + "..." if len(content) > 15 else content
                plt.annotate(snippet, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]),
                           fontsize=8, alpha=0.7)

        plt.title("2D Visualization of Node Embeddings using t-SNE")
        plt.xlabel("Dimension 1")
        plt.ylabel("Dimension 2")
        plt.tight_layout()
        plt.show()

    def plot_adjacency_matrix(self, figsize: Tuple[int, int] = (10, 8),
                             cmap: str = 'viridis'):
        """
        Visualize the adjacency matrix

        Args:
            figsize: Figure size (width, height)
            cmap: Colormap to use
        """
        plt.figure(figsize=figsize)
        plt.imshow(self.relv_db.adjacency_matrix, cmap=cmap)
        plt.colorbar(label='Similarity')
        plt.title("RelV Adjacency Matrix")
        plt.xlabel("Node Index")
        plt.ylabel("Node Index")
        plt.tight_layout()
        plt.show()

    def community_analysis(self, resolution: float = 1.0):
        """
        Analyze and visualize communities in the graph

        Args:
            resolution: Resolution parameter for community detection
        """
        try:
            # Detect communities using Louvain method
            from community import community_louvain

            partition = community_louvain.best_partition(self.relv_db.graph, resolution=resolution)

            # Count communities
            communities = {}
            for node, community_id in partition.items():
                if community_id not in communities:
                    communities[community_id] = []
                communities[community_id].append(node)

            print(f"Detected {len(communities)} communities")

            # Visualize with community colors
            plt.figure(figsize=(12, 8))
            pos = nx.spring_layout(self.relv_db.graph, seed=42)

            # Draw nodes colored by community
            nx.draw_networkx_nodes(
                self.relv_db.graph,
                pos,
                node_size=200,
                cmap=plt.cm.rainbow,
                node_color=list(partition.values())
            )

            # Draw edges
            nx.draw_networkx_edges(
                self.relv_db.graph,
                pos,
                alpha=0.5
            )

            plt.title("Community Detection in RelV Database")
            plt.axis('off')
            plt.tight_layout()
            plt.show()

            # Print summary of communities
            for comm_id, nodes in communities.items():
                print(f"Community {comm_id}: {len(nodes)} nodes")
                if len(nodes) > 0:
                    sample_node = nodes[0]
                    print(f"  Sample content: {self.relv_db.get_node_content(sample_node)[:200]}...")

        except ImportError:
            print("Community detection requires python-louvain package.")
            print("Install it with: pip install python-louvain")

# Example usage

# After setting up your RelV database:
visualizer = RelVVisualizer(relv_db)
visualizer.plot_graph()
visualizer.plot_embeddings_2d()
visualizer.plot_adjacency_matrix()
visualizer.community_analysis()

import numpy as np
from typing import Dict, List, Tuple, Optional, Set, Union, Callable
import heapq
from collections import defaultdict
import time

class RelVOptimizer:
    """
    Advanced optimization tools for RelV database to handle
    small context windows and large-scale information retrieval
    """

    def __init__(self, relv_db: 'RelV', context_manager: 'RelVContextManager'):
        """
        Initialize the optimizer

        Args:
            relv_db: RelV database instance
            context_manager: RelV context manager instance
        """
        self.relv_db = relv_db
        self.context_manager = context_manager

    def find_information_bottlenecks(self) -> List[str]:
        """
        Identify nodes that act as information bottlenecks in the graph
        These are critical nodes that connect different subclusters

        Returns:
            List of node IDs that serve as bottlenecks
        """
        # Calculate betweenness centrality
        import networkx as nx
        betweenness = nx.betweenness_centrality(self.relv_db.graph)

        # Sort nodes by betweenness
        sorted_nodes = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)

        # Return top 10% or at least 3 nodes, whichever is greater
        count = max(3, int(len(self.relv_db.nodes) * 0.1))
        return [node_id for node_id, _ in sorted_nodes[:count]]

    def create_summary_nodes(self, community_threshold: float = 0.5) -> Dict[str, str]:
        """
        Create summary nodes for dense communities to reduce context size

        Args:
            community_threshold: Threshold for community detection

        Returns:
            Dictionary mapping summary node IDs to lists of summarized node IDs
        """
        # First detect communities
        import networkx as nx
        from community import community_louvain

        # Detect communities
        partition = community_louvain.best_partition(self.relv_db.graph)

        # Group nodes by community
        communities = defaultdict(list)
        for node_id, community_id in partition.items():
            communities[community_id].append(node_id)

        # Only create summaries for communities with more than 3 nodes
        summaries = {}

        for community_id, node_ids in communities.items():
            if len(node_ids) < 3:
                continue

            # Collect content from all nodes in the community
            community_content = []
            for node_id in node_ids:
                community_content.append(self.relv_db.get_node_content(node_id))

            # Join content into a single text
            all_content = "\n\n".join(community_content)

            # Create a summary using the LLM
            summary_prompt = f"""Summarize the following information in a concise manner,
            preserving key facts and relationships:

            {all_content[:2000]}  # Limit to avoid context overflow

            Provide a short summary that captures the essential information.
            """

            response = self.context_manager.llm(summary_prompt, max_tokens=200, temperature=0.1)
            summary = response["choices"][0]["text"].strip()

            # Create a new node with the summary
            embedding = self.context_manager.embedding_model.encode(summary)

            summary_node_id = self.relv_db.add_node(
                content=summary,
                embedding=embedding,
                metadata={
                    "type": "summary",
                    "summarized_nodes": node_ids,
                    "community_id": community_id
                }
            )

            # Add to results
            summaries[summary_node_id] = node_ids

        return summaries

    def create_hierarchical_index(self, levels: int = 2) -> Dict:
        """
        Create a hierarchical index for the database to enable efficient navigation

        Args:
            levels: Number of hierarchical levels

        Returns:
            Dictionary representing the hierarchical structure
        """
        # Use clustering to create hierarchy
        from sklearn.cluster import AgglomerativeClustering

        # Collect all embeddings
        embeddings = []
        node_ids = []

        for node_id, node in self.relv_db.nodes.items():
            embeddings.append(node.embedding)
            node_ids.append(node_id)

        embeddings_array = np.array(embeddings)

        # Create the hierarchical structure
        hierarchy = {}

        # Bottom level is just the original nodes
        hierarchy[levels] = {i: [node_id] for i, node_id in enumerate(node_ids)}

        # Create higher levels through clustering
        for level in range(levels-1, 0, -1):
            # Number of clusters at this level
            n_clusters = max(1, len(node_ids) // (5 ** (levels - level)))

            # Apply clustering
            clustering = AgglomerativeClustering(
                n_clusters=min(n_clusters, len(node_ids)),
                metric='euclidean',
                linkage='ward'
            )

            cluster_labels = clustering.fit_predict(embeddings_array)

            # Group nodes by cluster
            clusters = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                clusters[label].extend(hierarchy[level+1][i])

            # Store in hierarchy
            hierarchy[level] = clusters

        # Create summary nodes for each cluster
        summaries = {}

        for level in range(1, levels):
            level_summaries = {}

            for cluster_id, cluster_node_ids in hierarchy[level].items():
                # Get content from all nodes in cluster
                cluster_content = []
                for node_id in cluster_node_ids:
                    if node_id in self.relv_db.nodes:  # Check if node exists
                        cluster_content.append(self.relv_db.get_node_content(node_id))

                if not cluster_content:
                    continue

                # Create summary
                all_content = "\n\n".join(cluster_content[:5])  # Limit to top 5 to avoid overflow

                summary_prompt = f"""Create a concise summary of the following related information:

                {all_content[:1500]}

                Provide a brief summary that captures the main themes and key points.
                """

                response = self.context_manager.llm(summary_prompt, max_tokens=150, temperature=0.1)
                summary = response["choices"][0]["text"].strip()

                # Create embedding and add to database
                embedding = self.context_manager.embedding_model.encode(summary)

                summary_node_id = self.relv_db.add_node(
                    content=summary,
                    embedding=embedding,
                    metadata={
                        "type": "hierarchy_summary",
                        "level": level,
                        "cluster_id": cluster_id,
                        "summarized_nodes": cluster_node_ids
                    }
                )

                level_summaries[cluster_id] = summary_node_id

            summaries[level] = level_summaries

        # Add summaries to hierarchy information
        hierarchy["summaries"] = summaries

        return hierarchy

    def optimize_query_path(self, query: str, max_context_size: int = 1024) -> List[str]:
        """
        Find optimal path through the knowledge graph for a specific query
        to fit within context window constraints

        Args:
            query: Query string
            max_context_size: Maximum context size in tokens

        Returns:
            List of node IDs forming the optimal path
        """
        # Get query embedding
        query_embedding = self.context_manager.embedding_model.encode(query)

        # Find starting point - most relevant node
        relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=3)
        if not relevant_nodes:
            return []

        start_node_id = relevant_nodes[0][0]

        # Use A* search to find optimal path
        # The goal is to maximize relevance while staying within context limits

        # Priority queue for A* search
        # Format: (priority, path_length, current_node, path, visited)
        queue = [(0, 0, start_node_id, [start_node_id], {start_node_id})]

        best_path = [start_node_id]
        best_score = self._calculate_path_relevance(best_path, query_embedding)

        max_iterations = 100  # Limit search iterations
        iterations = 0

        while queue and iterations < max_iterations:
            iterations += 1

            # Get highest priority path
            _, path_length, current, path, visited = heapq.heappop(queue)

            # Check if current path is better than best found so far
            current_score = self._calculate_path_relevance(path, query_embedding)
            current_size = self._estimate_context_size(path)

            if current_score > best_score and current_size <= max_context_size:
                best_path = path.copy()
                best_score = current_score

            # Check neighbors for potential expansions
            for neighbor in self.relv_db.graph.neighbors(current):
                if neighbor not in visited:
                    # Create new path including this neighbor
                    new_path = path + [neighbor]
                    new_visited = visited.union({neighbor})
                    new_size = self._estimate_context_size(new_path)

                    # Only consider if within context limits
                    if new_size <= max_context_size:
                        # Calculate path score (negative for min-heap)
                        neighbor_relevance = self._calculate_node_relevance(neighbor, query_embedding)
                        path_relevance = self._calculate_path_relevance(new_path, query_embedding)

                        # Heuristic: prioritize relevance and path coherence
                        priority = -1 * (path_relevance + 0.5 * neighbor_relevance)

                        heapq.heappush(
                            queue,
                            (priority, path_length + 1, neighbor, new_path, new_visited)
                        )

        return best_path

    def _calculate_node_relevance(self, node_id: str, query_embedding: np.ndarray) -> float:
        """Calculate relevance of a node to query"""
        node = self.relv_db.get_node(node_id)
        return self.relv_db._calculate_similarity(node.embedding, query_embedding)

    def _calculate_path_relevance(self, path: List[str], query_embedding: np.ndarray) -> float:
        """Calculate overall relevance of a path to query"""
        if not path:
            return 0.0

        # Weighted sum of individual node relevances
        relevances = [self._calculate_node_relevance(node_id, query_embedding) for node_id in path]

        # Recent nodes have higher weight
        weights = np.linspace(0.5, 1.0, len(relevances))

        return np.average(relevances, weights=weights)

    def _estimate_context_size(self, node_ids: List[str]) -> int:
        """Estimate token size of nodes"""
        # Rough estimate: 1 token per 4 characters
        total_chars = 0
        for node_id in node_ids:
            content = self.relv_db.get_node_content(node_id)
            total_chars += len(content)

        return total_chars // 4

    def create_information_pathway(self, start_query: str, end_query: str) -> List[str]:
        """
        Create an information pathway connecting two queries

        Args:
            start_query: Starting point query
            end_query: Destination query

        Returns:
            List of node IDs forming the pathway
        """
        # Get embeddings for both queries
        start_embedding = self.context_manager.embedding_model.encode(start_query)
        end_embedding = self.context_manager.embedding_model.encode(end_query)

        # Find most relevant nodes for each query
        start_nodes = self.relv_db.query_by_embedding(start_embedding, k=2)
        end_nodes = self.relv_db.query_by_embedding(end_embedding, k=2)

        if not start_nodes or not end_nodes:
            return []

        start_node_id = start_nodes[0][0]
        end_node_id = end_nodes[0][0]

        # Find shortest path in graph
        try:
            import networkx as nx
            path = nx.shortest_path(self.relv_db.graph, start_node_id, end_node_id)
            return path
        except (nx.NetworkXNoPath, nx.NodeNotFound):
            # No direct path, find intermediate nodes
            # Create a temporary connection to enable search
            bridge_embedding = (start_embedding + end_embedding) / 2

            # Find nodes similar to the bridge
            bridge_nodes = self.relv_db.query_by_embedding(bridge_embedding, k=3)

            if not bridge_nodes:
                return []

            # Try to connect through bridge nodes
            for bridge_id, _ in bridge_nodes:
                # Try path from start to bridge
                try:
                    path1 = nx.shortest_path(self.relv_db.graph, start_node_id, bridge_id)
                    # Try path from bridge to end
                    path2 = nx.shortest_path(self.relv_db.graph, bridge_id, end_node_id)
                    # Combine paths (remove duplicate bridge node)
                    return path1 + path2[1:]
                except (nx.NetworkXNoPath, nx.NodeNotFound):
                    continue

            # If all else fails, return disconnected important nodes
            return [start_node_id, end_node_id]

import time
import numpy as np
from typing import Dict, List, Tuple, Any, Callable
import matplotlib.pyplot as plt
from tqdm import tqdm

class RelVEvaluator:
    """Evaluation framework for RelV database performance"""

    def __init__(self, relv_db: 'RelV', context_manager: 'RelVContextManager'):
        """
        Initialize the evaluator

        Args:
            relv_db: RelV database instance
            context_manager: RelV context manager instance
        """
        self.relv_db = relv_db
        self.context_manager = context_manager
        self.results = {}

    def benchmark_retrieval_speed(self, query_count: int = 20) -> Dict:
        """
        Benchmark the speed of retrieval operations

        Args:
            query_count: Number of random queries to test

        Returns:
            Dictionary with benchmark results
        """
        # Generate random query embeddings
        np.random.seed(42)
        query_embeddings = np.random.rand(query_count, self.relv_db.embedding_dim)

        # Normalize embeddings to unit length
        for i in range(query_count):
            query_embeddings[i] = query_embeddings[i] / np.linalg.norm(query_embeddings[i])

        # Test regular retrieval
        regular_times = []
        for i in range(query_count):
            start_time = time.time()
            _ = self.relv_db.query_by_embedding(query_embeddings[i], k=5)
            end_time = time.time()
            regular_times.append(end_time - start_time)

        avg_regular_time = np.mean(regular_times)
        print(f"Average retrieval time: {avg_regular_time:.5f} seconds")

        # Test retrieval with path optimization if at least 10 nodes
        if len(self.relv_db.nodes) >= 10:
            optimizer = RelVOptimizer(self.relv_db, self.context_manager)

            optimized_times = []
            for i in range(query_count):
                start_time = time.time()
                _ = optimizer.optimize_query_path("Test query " + str(i))
                end_time = time.time()
                optimized_times.append(end_time - start_time)

            avg_optimized_time = np.mean(optimized_times)
            print(f"Average optimized path time: {avg_optimized_time:.5f} seconds")
        else:
            avg_optimized_time = None

        results = {
            "average_retrieval_time": avg_regular_time,
            "average_optimized_time": avg_optimized_time,
            "node_count": len(self.relv_db.nodes),
            "edge_count": self.relv_db.graph.number_of_edges()
        }

        self.results["retrieval_speed"] = results
        return results

    def evaluate_contextual_relevance(self, test_queries: List[str],
                                     ground_truth: Dict[str, List[str]] = None) -> Dict:
        """
        Evaluate the relevance of retrieved context

        Args:
            test_queries: List of test queries
            ground_truth: Optional mapping of query to relevant node IDs

        Returns:
            Dictionary with evaluation results
        """
        results = {
            "query_results": {},
            "overall_stats": {}
        }

        similarities = []

        for query in test_queries:
            # Get query embedding
            query_embedding = self.context_manager.embedding_model.encode(query)

            # Get relevant nodes
            relevant_nodes = self.relv_db.query_by_embedding(query_embedding, k=5)

            # Calculate average similarity
            avg_similarity = np.mean([sim for _, sim in relevant_nodes])
            similarities.append(avg_similarity)

            # Store individual results
            results["query_results"][query] = {
                "retrieved_nodes": relevant_nodes,
                "average_similarity": avg_similarity
            }

            # Check against ground truth if provided
            if ground_truth and query in ground_truth:
                true_relevant = set(ground_truth[query])
                retrieved = set([node_id for node_id, _ in relevant_nodes])

                # Calculate metrics
                intersection = true_relevant.intersection(retrieved)
                precision = len(intersection) / len(retrieved) if retrieved else 0
                recall = len(intersection) / len(true_relevant) if true_relevant else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

                results["query_results"][query].update({
                    "precision": precision,
                    "recall": recall,
                    "f1": f1
                })

        # Calculate overall stats
        results["overall_stats"]["average_similarity"] = np.mean(similarities)

        if ground_truth:
            overall_precision = np.mean([r.get("precision", 0) for r in results["query_results"].values()])
            overall_recall = np.mean([r.get("recall", 0) for r in results["query_results"].values()])
            overall_f1 = np.mean([r.get("f1", 0) for r in results["query_results"].values()])

            results["overall_stats"].update({
                "precision": overall_precision,
                "recall": overall_recall,
                "f1": overall_f1
            })

        self.results["contextual_relevance"] = results
        return results

    def evaluate_slm_performance(self, test_queries: List[str],
                               evaluate_fn: Callable = None) -> Dict:
        """
        Evaluate performance improvement for small language models

        Args:
            test_queries: List of test queries
            evaluate_fn: Optional function to evaluate response quality

        Returns:
            Dictionary with evaluation results
        """
        results = {
            "queries": {},
            "overall": {}
        }

        # Run queries with and without RelV context enhancement
        for query in tqdm(test_queries):
            # Without RelV context (just direct query)
            start_time = time.time()
            direct_response = self.context_manager.llm(query, max_tokens=256, temperature=0.1)
            direct_time = time.time() - start_time
            direct_text = direct_response["choices"][0]["text"]

            # With RelV context enhancement
            start_time = time.time()
            enhanced_response = self.context_manager.query(query, k=3)
            enhanced_time = time.time() - start_time

            # Store results
            results["queries"][query] = {
                "direct_response": direct_text,
                "enhanced_response": enhanced_response,
                "direct_time": direct_time,
                "enhanced_time": enhanced_time
            }

            # Use evaluation function if provided
            if evaluate_fn:
                direct_score = evaluate_fn(query, direct_text)
                enhanced_score = evaluate_fn(query, enhanced_response)

                results["queries"][query]["direct_score"] = direct_score
                results["queries"][query]["enhanced_score"] = enhanced_score
                results["queries"][query]["improvement"] = enhanced_score - direct_score

        # Calculate overall stats
        direct_times = [r["direct_time"] for r in results["queries"].values()]
        enhanced_times = [r["enhanced_time"] for r in results["queries"].values()]

        results["overall"]["avg_direct_time"] = np.mean(direct_times)
        results["overall"]["avg_enhanced_time"] = np.mean(enhanced_times)
        results["overall"]["time_overhead"] = np.mean(enhanced_times) - np.mean(direct_times)

        if evaluate_fn:
            improvements = [r["improvement"] for r in results["queries"].values()]
            results["overall"]["avg_improvement"] = np.mean(improvements)
            results["overall"]["improvement_stddev"] = np.std(improvements)

        self.results["slm_performance"] = results
        return results

    def evaluate_memory_efficiency(self, node_counts: List[int]) -> Dict:
        """
        Evaluate memory usage efficiency at different scales

        Args:
            node_counts: List of node counts to test

        Returns:
            Dictionary with memory evaluation results
        """
        import os
        import psutil
        import gc

        results = {
            "memory_usage": [],
            "adjacency_size": [],
            "graph_size": []
        }

        process = psutil.Process(os.getpid())

        # Test at different scales
        current_nodes = len(self.relv_db.nodes)

        for target_count in node_counts:
            if target_count <= current_nodes:
                # Measure current memory usage
                gc.collect()
                memory_usage = process.memory_info().rss / (1024 * 1024)  # MB

                # Estimate adjacency matrix size
                adj_size = self.relv_db.adjacency_matrix.size * self.relv_db.adjacency_matrix.itemsize
                adj_size_mb = adj_size / (1024 * 1024)

                results["memory_usage"].append(memory_usage)
                results["adjacency_size"].append(adj_size_mb)
                results["graph_size"].append(self.relv_db.graph.number_of_edges())

            else:
                print(f"Skipping node count {target_count}, exceeds current DB size")

        # Plot results
        if len(results["memory_usage"]) > 1:
            fig, axs = plt.subplots(1, 2, figsize=(14, 6))

            # Memory usage plot
            axs[0].plot(node_counts[:len(results["memory_usage"])], results["memory_usage"],
                      marker='o', label='Total Memory')
            axs[0].plot(node_counts[:len(results["adjacency_size"])], results["adjacency_size"],
                      marker='s', label='Adjacency Matrix')

            axs[0].set_xlabel('Number of Nodes')
            axs[0].set_ylabel('Memory Usage (MB)')
            axs[0].set_title('Memory Scaling')
            axs[0].legend()

            # Graph edges plot
            axs[1].plot(node_counts[:len(results["graph_size"])], results["graph_size"],
                      marker='o', color='green')
            axs[1].set_xlabel('Number of Nodes')
            axs[1].set_ylabel('Number of Edges')
            axs[1].set_title('Graph Size Scaling')

            plt.tight_layout()
            plt.show()

        self.results["memory_efficiency"] = results
        return results

    def plot_results(self):
        """Plot all evaluation results"""
        if not self.results:
            print("No evaluation results available")
            return

        # Create a figure with subplots based on available results
        num_plots = len(self.results)
        fig, axs = plt.subplots(num_plots, 1, figsize=(10, 5 * num_plots))

        if num_plots == 1:
            axs = [axs]  # Make iterable if only one subplot

        plot_idx = 0

        # Plot retrieval speed results
        if "retrieval_speed" in self.results:
            retrieval = self.results["retrieval_speed"]

            times = [retrieval["average_retrieval_time"]]
            labels = ["Standard"]

            if retrieval["average_optimized_time"]:
                times.append(retrieval["average_optimized_time"])
                labels.append("Optimized")

            axs[plot_idx].bar(labels, times, color=['blue', 'orange'])
            axs[plot_idx].set_ylabel('Time (seconds)')
            axs[plot_idx].set_title('Retrieval Speed Comparison')

            plot_idx += 1

        # Plot contextual relevance
        if "contextual_relevance" in self.results:
            relevance = self.results["contextual_relevance"]

            if "overall_stats" in relevance:
                stats = relevance["overall_stats"]

                if all(m in stats for m in ["precision", "recall", "f1"]):
                    metrics = ["precision", "recall", "f1"]
                    values = [stats[m] for m in metrics]

                    axs[plot_idx].bar(metrics, values, color=['green', 'red', 'purple'])
                    axs[plot_idx].set_ylabel('Score')
                    axs[plot_idx].set_title('Relevance Metrics')
                    axs[plot_idx].set_ylim(0, 1)

                    plot_idx += 1

        # Plot SLM performance improvement
        if "slm_performance" in self.results:
            perf = self.results["slm_performance"]

            if "overall" in perf and "avg_improvement" in perf["overall"]:
                improvement = perf["overall"]["avg_improvement"]

                axs[plot_idx].bar(["Performance Improvement"], [improvement],
                               color='blue' if improvement >= 0 else 'red')
                axs[plot_idx].set_title('Average Performance Improvement')
                axs[plot_idx].set_ylabel('Score Difference')

                plot_idx += 1

        plt.tight_layout()
        plt.show()

"""PDF and OCR Document Processing Pipeline for RelV"""

!pip install pymupdf pdf2image pytesseract pillow

!pip install opencv-python

import fitz  # PyMuPDF
from pdf2image import convert_from_path
import pytesseract
from PIL import Image
import os

def is_text_pdf(pdf_path, sample_pages=3):
    """
    Checks if a PDF contains embedded text by scanning the first few pages.

    Args:
        pdf_path (str): Path to the PDF file.
        sample_pages (int): Number of pages to check for embedded text.

    Returns:
        bool: True if embedded text is detected, False otherwise.
    """
    doc = fitz.open(pdf_path)  # Open PDF
    for page_num in range(min(sample_pages, len(doc))):  # Check first few pages
        text = doc[page_num].get_text().strip()
        if text:  # If text is found, return True
            return True
    return False  # No text found, assume it's an image-based PDF

import cv2
import numpy as np

def preprocess_image(image):
    """
    Applies preprocessing techniques to improve OCR accuracy.

    Args:
        image (PIL.Image): Input image.

    Returns:
        preprocessed_img (OpenCV Image): Processed image.
    """
    # Convert PIL Image to OpenCV format
    img = np.array(image)
    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert to grayscale

    # Apply noise reduction (Gaussian Blur)
    img = cv2.GaussianBlur(img, (5, 5), 0)

    # Apply adaptive thresholding for better contrast
    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, 11, 2)

    # Deskew image
    coords = np.column_stack(np.where(img > 0))  # Find text coordinates
    angle = cv2.minAreaRect(coords)[-1]  # Get the angle
    if angle < -45:
        angle = -(90 + angle)
    else:
        angle = -angle

    (h, w) = img.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)  # Compute rotation matrix
    img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC,
                         borderMode=cv2.BORDER_REPLICATE)  # Rotate image

    return img

def compute_noise_score(image):
    """
    Computes a noise score based on image sharpness and contrast.

    Args:
        image (PIL.Image): Input image.

    Returns:
        float: Noise score (higher = less noisy, lower = more noise).
    """
    # Convert PIL Image to OpenCV format (grayscale)
    img = np.array(image.convert('L'))

    # 1. Sharpness measurement using Laplacian variance
    laplacian_var = cv2.Laplacian(img, cv2.CV_64F).var()

    # 2. Contrast measurement using RMS contrast
    rms_contrast = np.std(img)

    # Combine the two scores (adjust weighting as needed)
    noise_score = (laplacian_var * 0.7) + (rms_contrast * 0.3)

    return noise_score

def extract_text_pymupdf(pdf_path):
    """
    Extracts text from a text-based PDF using PyMuPDF.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        str: Extracted text.
    """
    doc = fitz.open(pdf_path)
    text = "\n".join([page.get_text() for page in doc])  # Extract text from all pages
    return text

def extract_text_ocr(pdf_path, noise_threshold=100):
    """
    Extracts text from an image-based PDF using OCR with optional preprocessing.

    Args:
        pdf_path (str): Path to the PDF file.
        noise_threshold (float): Minimum score required to skip preprocessing.

    Returns:
        str: Extracted text.
    """
    images = convert_from_path(pdf_path)  # Convert PDF pages to images
    extracted_text = ""

    for image in images:
        noise_score = compute_noise_score(image)
        print(f"Noise Score: {noise_score}")

        if noise_score < noise_threshold:
            print("Applying preprocessing due to low noise score...")
            processed_img = preprocess_image(image)  # Apply preprocessing
        else:
            print("Skipping preprocessing...")
            processed_img = np.array(image.convert('L'))  # Use original image

        text = pytesseract.image_to_string(processed_img)  # Perform OCR
        extracted_text += text + "\n"

    return extracted_text

def process_pdf(pdf_path):
    """
    Determines whether a PDF contains embedded text or is image-based,
    then extracts text accordingly.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        str: Extracted text.
    """
    if is_text_pdf(pdf_path):
        print("PDF contains embedded text. Using PyMuPDF for extraction...")
        return extract_text_pymupdf(pdf_path)
    else:
        print("PDF appears to be image-based. Using OCR for extraction...")
        return extract_text_ocr(pdf_path)

!apt-get update
!apt-get install poppler-utils

!apt-get install tesseract-ocr
!apt-get install libtesseract-dev

"""PDF IMPORT or OCR IMPORT CELL:"""

"""Scanned PDF Example"""
pdf_path = "/content/LenderFeesWorksheetNew.pdf"  # Replace with actual PDF file path
extracted_text_pdf = process_pdf(pdf_path)
print(extracted_text_pdf[:1000])  # Print first 1000 characters to check output

"""OCR Example"""
pdf_path = "/content/QCD_5797-882.pdf"  # Replace with actual PDF file path
extracted_text_ocr = process_pdf(pdf_path)
print(extracted_text_ocr[:1000])  # Print first 1000 characters to check output

# Assuming you have the extracted text in variables named
# 'extracted_text_ocr' and 'extracted_text_pdf'

# Add extracted texts to RelV database
print("Adding extracted texts to RelV database...")

# Add OCR text
ocr_nodes = context_manager.add_document(extracted_text_ocr, metadata={"source": "OCR"})
print(f"Added {len(ocr_nodes)} nodes from OCR text")

# Add PDF text
pdf_nodes = context_manager.add_document(extracted_text_pdf, metadata={"source": "PDF"})
print(f"Added {len(pdf_nodes)} nodes from PDF text")

print("Finished adding extracted texts to RelV database.")

import tkinter as tk
from tkinter import ttk, filedialog  # Import ttk for modern widgets and filedialog for file selection

# ... (Your RelV, RelVContextManager, and RelVVisualizer classes) ...

class RelVApp:
    def __init__(self, master):
        self.master = master
        master.title("RelV Database GUI")

        # Initialize RelV components
        self.relv_db = RelV()
        self.context_manager = RelVContextManager(self.relv_db, llm, embedding_model)
        self.visualizer = RelVVisualizer(self.relv_db)

        # Create notebook (tabbed interface)
        self.notebook = ttk.Notebook(master)
        self.notebook.pack(expand=True, fill="both")

        # Create tabs
        self.create_visualization_tab()
        self.create_document_tab()
        self.create_chat_tab()

    def create_visualization_tab(self):
        viz_frame = ttk.Frame(self.notebook)
        self.notebook.add(viz_frame, text="Visualization")

        # Add buttons/controls for different visualizations
        graph_button = ttk.Button(viz_frame, text="Plot Graph", command=self.visualizer.plot_graph)
        graph_button.pack(pady=10)

        embeddings_button = ttk.Button(viz_frame, text="Plot Embeddings", command=self.visualizer.plot_embeddings_2d)
        embeddings_button.pack(pady=10)

        # ... (Add other visualization buttons as needed) ...

    def create_document_tab(self):
        doc_frame = ttk.Frame(self.notebook)
        self.notebook.add(doc_frame, text="Document Manager")

        # Add controls for uploading and managing documents
        upload_button = ttk.Button(doc_frame, text="Upload Document", command=self.upload_document)
        upload_button.pack(pady=10)

        # ... (Add other document management controls) ...

    def create_chat_tab(self):
        chat_frame = ttk.Frame(self.notebook)
        self.notebook.add(chat_frame, text="LLM Chat")

        # Add chat input and output areas
        self.chat_input = tk.Text(chat_frame, height=5, width=50)
        self.chat_input.pack(pady=10)

        self.chat_output = tk.Text(chat_frame, height=10, width=50, state="disabled")
        self.chat_output.pack(pady=10)

        send_button = ttk.Button(chat_frame, text="Send", command=self.send_query)
        send_button.pack(pady=10)

    def upload_document(self):
        file_path = filedialog.askopenfilename(title="Select Document")
        if file_path:
            with open(file_path, "r") as file:
                document = file.read()
                self.context_manager.add_document(document)
            print(f"Document uploaded: {file_path}")

    def send_query(self):
        query = self.chat_input.get("1.0", tk.END).strip()
        if query:
            response = self.context_manager.query(query)
            self.chat_output.config(state="normal")
            self.chat_output.insert(tk.END, f"User: {query}\n")
            self.chat_output.insert(tk.END, f"Assistant: {response}\n")
            self.chat_output.config(state="disabled")
            self.chat_input.delete("1.0", tk.END)

# Create the main window and run the app
root = tk.Tk()
app = RelVApp(root)
root.mainloop()